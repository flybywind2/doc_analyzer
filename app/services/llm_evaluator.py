"""
LLM Evaluator Service
"""
import uuid
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from app.config import settings
from app.models.application import Application
from app.models.evaluation import EvaluationCriteria, EvaluationHistory


class LLMEvaluator:
    """LLM-based application evaluator"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            base_url=settings.llm_api_base_url,
            api_key=settings.llm_api_key,
            model=settings.llm_model_name,
            temperature=0.1,  # ÏùºÍ¥ÄÏÑ± ÌôïÎ≥¥
            default_headers={
                "x-dep-ticket": settings.llm_credential_key,
                "Send-System-Name": settings.llm_system_name,
                "User-ID": settings.llm_user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )
    
    def build_evaluation_prompt(
        self, 
        application: Application, 
        criteria_list: List[EvaluationCriteria]
    ) -> str:
        """
        Build evaluation prompt for LLM
        
        Args:
            application: Application to evaluate
            criteria_list: List of evaluation criteria
            
        Returns:
            Formatted prompt string
        """
        # Í≥ºÏ†ú Ï†ïÎ≥¥ Íµ¨ÏÑ±
        app_info = f"""
# AI Í≥ºÏ†ú ÏßÄÏõêÏÑú ÌèâÍ∞Ä

## Í≥ºÏ†ú Í∏∞Î≥∏ Ï†ïÎ≥¥
- Í≥ºÏ†úÎ™Ö: {application.subject or 'N/A'}
- ÏÇ¨ÏóÖÎ∂Ä: {application.division or 'N/A'}
- Ï∞∏Ïó¨ Ïù∏Ïõê: {application.participant_count or 'N/A'}Î™Ö
- ÎåÄÌëúÏûê: {application.representative_name or 'N/A'}

## Ïã†Ï≤≠ ÎÇ¥Ïö©
### ÌòÑÏû¨ ÏóÖÎ¨¥
{application.current_work or 'N/A'}

### Pain Point (Ìï¥Í≤∞ÌïòÍ≥†Ïûê ÌïòÎäî Î¨∏Ï†ú)
{application.pain_point or 'N/A'}

### Í∞úÏÑ† ÏïÑÏù¥ÎîîÏñ¥
{application.improvement_idea or 'N/A'}

### Í∏∞ÎåÄ Ìö®Í≥º
{application.expected_effect or 'N/A'}

### Î∞îÎùºÎäî Ï†ê
{application.hope or 'N/A'}

## ÏÇ¨Ï†Ñ ÏÑ§Î¨∏
{json.dumps(application.pre_survey, ensure_ascii=False, indent=2) if application.pre_survey else 'N/A'}

## Ï∞∏Ïó¨Ïûê Í∏∞Ïà† Ïó≠Îüâ
{json.dumps(application.tech_capabilities, ensure_ascii=False, indent=2) if application.tech_capabilities else 'N/A'}

---

## ÌèâÍ∞Ä Í∏∞Ï§Ä ({len(criteria_list)}Í∞ú Ìï≠Î™©)
"""
        
        for i, criteria in enumerate(criteria_list, 1):
            app_info += f"""
{i}. **{criteria.name}** (Í∞ÄÏ§ëÏπò: {criteria.weight})
   - {criteria.description}
   - ÌèâÍ∞Ä Í∞ÄÏù¥Îìú: {criteria.evaluation_guide}
"""
        
        prompt = f"""{app_info}

---

## ÌèâÍ∞Ä ÏßÄÏπ®
1. Í∞Å ÌèâÍ∞Ä Í∏∞Ï§ÄÏóê ÎåÄÌï¥ **ÏßÄÏõêÏÑúÏóê Î™ÖÏãúÎêú ÎÇ¥Ïö©ÏùÑ Ïö∞ÏÑ†**ÏúºÎ°ú ÌèâÍ∞ÄÌïòÏÑ∏Ïöî.
2. Î™ÖÏãúÎêòÏßÄ ÏïäÏïòÏßÄÎßå Ï∂îÎ°† Í∞ÄÎä•Ìïú Í≤ΩÏö∞, **"[AI Ï∂îÎ°†]"** ÌëúÏãúÎ•º Î™ÖÌôïÌûà ÌïòÏÑ∏Ïöî.
3. Í∞Å Ìï≠Î™©ÏùÑ S/A/B/C/D 5Îã®Í≥ÑÎ°ú ÌèâÍ∞ÄÌïòÏÑ∏Ïöî:
   - S: Îß§Ïö∞ Ïö∞Ïàò (5Ï†ê)
   - A: Ïö∞Ïàò (4Ï†ê)
   - B: Î≥¥ÌÜµ (3Ï†ê)
   - C: ÎØ∏Ìù° (2Ï†ê)
   - D: Îß§Ïö∞ ÎØ∏Ìù° (1Ï†ê)

4. Í∞ÄÏ§ëÏπòÎ•º Î∞òÏòÅÌïòÏó¨ Ï¢ÖÌï© Îì±Í∏âÏùÑ ÏÇ∞Ï∂úÌïòÏÑ∏Ïöî.
5. Í≥ºÏ†ú ÏöîÏïΩÏùÄ **3-5Í∞ú bullet point** ÌòïÌÉúÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.

## ÏùëÎãµ ÌòïÏãù (JSON)
Îã§Ïùå JSON ÌòïÏãùÏúºÎ°ú Ï†ïÌôïÌûà ÏùëÎãµÌïòÏÑ∏Ïöî:

{{
  "evaluation_detail": {{
    "Í≤ΩÏòÅÏÑ±Í≥º": {{"grade": "A", "score": 4, "comment": "Íµ¨Ï≤¥Ï†ÅÏù∏ ÌèâÍ∞Ä ÎÇ¥Ïö©..."}},
    "Ï†ÑÎûµÍ≥ºÏ†ú Ïú†ÏÇ¨ÎèÑ": {{"grade": "B", "score": 3, "comment": "Íµ¨Ï≤¥Ï†ÅÏù∏ ÌèâÍ∞Ä ÎÇ¥Ïö©..."}},
    ...
  }},
  "overall_grade": "A",
  "overall_score": 4.2,
  "summary": "- Bullet point 1\\n- Bullet point 2\\n- Bullet point 3"
}}

**Ï§ëÏöî: Î∞òÎìúÏãú Ïú†Ìö®Ìïú JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌïòÏÑ∏Ïöî.**
"""
        return prompt
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type(Exception)
    )
    def evaluate_with_llm(self, prompt: str) -> Dict[str, Any]:
        """
        Evaluate application using LLM with retry logic
        
        Args:
            prompt: Evaluation prompt
            
        Returns:
            Evaluation result dictionary
            
        Raises:
            Exception: If evaluation fails after retries
        """
        response = self.llm.invoke(prompt)
        content = response.content
        
        # JSON ÌååÏã± ÏãúÎèÑ
        try:
            # Markdown code block Ï†úÍ±∞
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            result = json.loads(content.strip())
            return result
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON parsing error: {e}")
            print(f"Response content: {content}")
            raise
    
    def calculate_overall_grade(self, evaluation_detail: Dict[str, Any]) -> str:
        """
        Calculate overall grade from evaluation details
        
        Args:
            evaluation_detail: Dictionary of evaluation scores
            
        Returns:
            Overall grade (S/A/B/C/D)
        """
        total_score = 0
        count = 0
        
        for item in evaluation_detail.values():
            if isinstance(item, dict) and "score" in item:
                total_score += item["score"]
                count += 1
        
        if count == 0:
            return "C"
        
        avg_score = total_score / count
        
        if avg_score >= 4.5:
            return "S"
        elif avg_score >= 3.5:
            return "A"
        elif avg_score >= 2.5:
            return "B"
        elif avg_score >= 1.5:
            return "C"
        else:
            return "D"
    
    def evaluate_application(
        self, 
        db: Session, 
        application: Application,
        criteria_list: Optional[List[EvaluationCriteria]] = None
    ) -> bool:
        """
        Evaluate single application
        
        Args:
            db: Database session
            application: Application to evaluate
            criteria_list: Evaluation criteria (optional, will fetch if None)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get evaluation criteria if not provided
            if criteria_list is None:
                criteria_list = db.query(EvaluationCriteria).filter(
                    EvaluationCriteria.is_active == True
                ).order_by(EvaluationCriteria.display_order).all()
            
            if not criteria_list:
                print("‚ùå No evaluation criteria found")
                return False
            
            # Build prompt
            prompt = self.build_evaluation_prompt(application, criteria_list)
            
            # Evaluate with LLM
            print(f"ü§ñ Evaluating application {application.id} ({application.subject})...")
            result = self.evaluate_with_llm(prompt)
            
            # Extract results
            evaluation_detail = result.get("evaluation_detail", {})
            overall_grade = result.get("overall_grade") or self.calculate_overall_grade(evaluation_detail)
            summary = result.get("summary", "")
            
            # Update application
            application.ai_evaluation_detail = evaluation_detail
            application.ai_grade = overall_grade
            application.ai_summary = summary
            application.ai_evaluated_at = datetime.utcnow()
            application.status = "ai_evaluated"
            
            # Save evaluation history
            history = EvaluationHistory(
                application_id=application.id,
                evaluator_id=None,
                evaluator_type="AI",
                grade=overall_grade,
                summary=summary,
                evaluation_detail=evaluation_detail,
                ai_categories=application.ai_categories
            )
            db.add(history)
            
            db.commit()
            print(f"‚úÖ Application {application.id} evaluated: {overall_grade}")
            return True
            
        except Exception as e:
            print(f"‚ùå Error evaluating application {application.id}: {e}")
            db.rollback()
            return False


# Singleton instance
llm_evaluator = LLMEvaluator()

"""
LLM Evaluator Service
"""
import re
import uuid
import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from app.config import settings
from app.models.application import Application
from app.models.evaluation import EvaluationCriteria, EvaluationHistory
from app.services.rate_limiter import RateLimiter


class LLMEvaluator:
    """LLM-based application evaluator with ensemble support"""

    def __init__(self):
        # Primary LLM (A)
        self.llm_a = ChatOpenAI(
            base_url=settings.llm_api_base_url,
            api_key=settings.llm_api_key,
            model=settings.llm_model_name,
            temperature=0.1,  # ì¼ê´€ì„± í™•ë³´
            default_headers={
                "x-dep-ticket": settings.llm_credential_key,
                "Send-System-Name": settings.llm_system_name,
                "User-ID": settings.llm_user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )

        # Secondary LLM (B) - Optional for ensemble
        self.llm_b = None
        if settings.llm_b_api_base_url and settings.llm_b_api_key:
            self.llm_b = ChatOpenAI(
                base_url=settings.llm_b_api_base_url,
                api_key=settings.llm_b_api_key,
                model=settings.llm_b_model_name or settings.llm_model_name,
                temperature=0.1,
                default_headers={
                    "x-dep-ticket": settings.llm_b_credential_key or settings.llm_credential_key,
                    "Send-System-Name": settings.llm_system_name,
                    "User-ID": settings.llm_user_id,
                    "User-Type": "AD",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                },
            )
            print(f"âœ… Ensemble mode enabled: LLM A ({settings.llm_model_name}) + LLM B ({settings.llm_b_model_name or settings.llm_model_name})")
        else:
            print(f"â„¹ï¸  Single LLM mode: {settings.llm_model_name}")

        # Rate limiter: 20 calls per minute
        self.rate_limiter = RateLimiter(max_calls=20, time_window=60)

        # Criteria name to key mapping (í•œê¸€ -> ì˜ë¬¸)
        self.criteria_key_map = {
            "í˜ì‹ ì„±": "innovation",
            "ì‹¤í˜„ê°€ëŠ¥ì„±": "feasibility",
            "íš¨ê³¼ì„±": "impact",
            "ëª…í™•ì„±": "clarity"
        }

    def _build_criteria_guide(self, criteria_list: List[EvaluationCriteria]) -> str:
        """
        Build evaluation criteria guide from database criteria

        Args:
            criteria_list: List of evaluation criteria from DB

        Returns:
            Formatted criteria guide string
        """
        if not criteria_list:
            # Fallback to default if no criteria
            return """
**í˜ì‹ ì„± (Innovation)**: AI ê¸°ìˆ ì˜ ì°½ì˜ì„±ê³¼ ìƒˆë¡œì›€ (1-5ì )
**ì‹¤í˜„ê°€ëŠ¥ì„± (Feasibility)**: ê¸°ìˆ ì  êµ¬í˜„ ë‚œì´ë„ì™€ íŒ€ ì—­ëŸ‰ (1-5ì )
**íš¨ê³¼ì„± (Impact)**: ì¡°ì§ì— ë¯¸ì¹˜ëŠ” ê²½ì˜ íš¨ê³¼ (1-5ì )
**ëª…í™•ì„± (Clarity)**: ë¬¸ì œ ì •ì˜ì™€ í•´ê²° ë°©ì•ˆì˜ êµ¬ì²´ì„± (1-5ì )
""".strip()

        guide_parts = []
        for criteria in criteria_list:
            key = self.criteria_key_map.get(criteria.name, criteria.name.lower())
            guide_parts.append(f"""
**{criteria.name} ({key.capitalize()})**: {criteria.description}

{criteria.evaluation_guide}
""".strip())

        return "\n\n".join(guide_parts)

    def _build_json_format_example(self, criteria_list: List[EvaluationCriteria]) -> str:
        """
        Build JSON format example from database criteria

        Args:
            criteria_list: List of evaluation criteria from DB

        Returns:
            Formatted JSON example string
        """
        if not criteria_list:
            # Fallback to default
            criteria_list = [
                type('obj', (object,), {'name': 'í˜ì‹ ì„±', 'description': 'AI ê¸°ìˆ ì˜ ì°½ì˜ì„±ê³¼ ìƒˆë¡œì›€'})(),
                type('obj', (object,), {'name': 'ì‹¤í˜„ê°€ëŠ¥ì„±', 'description': 'ê¸°ìˆ ì  êµ¬í˜„ ë‚œì´ë„ì™€ íŒ€ ì—­ëŸ‰'})(),
                type('obj', (object,), {'name': 'íš¨ê³¼ì„±', 'description': 'ì¡°ì§ì— ë¯¸ì¹˜ëŠ” ê²½ì˜ íš¨ê³¼'})(),
                type('obj', (object,), {'name': 'ëª…í™•ì„±', 'description': 'ë¬¸ì œ ì •ì˜ì™€ í•´ê²° ë°©ì•ˆì˜ êµ¬ì²´ì„±'})()
            ]

        json_parts = []
        for criteria in criteria_list:
            key = self.criteria_key_map.get(criteria.name, criteria.name.lower())
            json_parts.append(f'''    "{key}": {{
      "score": 1-5 ì‚¬ì´ì˜ ì •ìˆ˜,
      "rationale": "{criteria.name} í‰ê°€ ê·¼ê±° (2-3ë¬¸ìž¥, ì§€ì›ì„œ ê¸°ë°˜)"
    }}''')

        return ",\n".join(json_parts)

    def build_evaluation_prompt(
        self, 
        application: Application, 
        criteria_list: List[EvaluationCriteria]
    ) -> str:
        """
        Build evaluation prompt for LLM
        
        Args:
            application: Application to evaluate
            criteria_list: List of evaluation criteria
            
        Returns:
            Formatted prompt string
        """
        # ê³¼ì œ ì •ë³´ êµ¬ì„±
        department_info = f"{application.division or 'N/A'} > {application.department.name if application.department else 'N/A'}"
        
        system_prompt = f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ëŒ€ê¸°ì—…ì˜ AI ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì¡°ì§: {department_info}

ì—­í• : ì§€ì›ì„œ ë‚´ìš©ì„ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
2. {department_info} ì¡°ì§ì˜ ì—…ë¬´ íŠ¹ì„±ì„ ê³ ë ¤í•œ í•´ì„
3. ì‚¬ì‹¤ ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
4. ê³¼ìž¥í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
"""

        app_info = f"""
# AI ê³¼ì œ ì§€ì›ì„œ í‰ê°€

## ê³¼ì œ ê¸°ë³¸ ì •ë³´
- ê³¼ì œëª…: {application.subject or 'N/A'}
- ì¡°ì§: {department_info}
- ì°¸ì—¬ ì¸ì›: {application.participant_count or 'N/A'}ëª…
- ëŒ€í‘œìž: {application.representative_name or 'N/A'}

## ì‹ ì²­ ë‚´ìš©
### í˜„ìž¬ ì—…ë¬´
{application.current_work or 'N/A'}

### Pain Point (í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œ)
{application.pain_point or 'N/A'}

### ê°œì„  ì•„ì´ë””ì–´
{application.improvement_idea or 'N/A'}

### ê¸°ëŒ€ íš¨ê³¼
{application.expected_effect or 'N/A'}

### ë°”ë¼ëŠ” ì 
{application.hope or 'N/A'}

## ì‚¬ì „ ì„¤ë¬¸
{json.dumps(application.pre_survey, ensure_ascii=False, indent=2) if application.pre_survey else 'N/A'}

## ì°¸ì—¬ìž ê¸°ìˆ  ì—­ëŸ‰
{json.dumps(application.tech_capabilities, ensure_ascii=False, indent=2) if application.tech_capabilities else 'N/A'}

---

## í‰ê°€ ìš”ì²­ì‚¬í•­

ì§€ì›ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìš”ì•½í•˜ê³  í‰ê°€í•˜ì„¸ìš”:

### 1. AI ê¸°ìˆ  ë¶„ë¥˜
ì§€ì›ì„œì—ì„œ ì–¸ê¸‰ëœ AI ê¸°ìˆ ì„ ë‹¤ìŒ ì¤‘ **í•˜ë‚˜ë§Œ** ì„ íƒí•˜ì„¸ìš”:
- **ì˜ˆì¸¡**: ë¯¸ëž˜ ê°’ ì˜ˆì¸¡, ìˆ˜ìš” ì˜ˆì¸¡, íŠ¸ë Œë“œ ë¶„ì„
- **ë¶„ë¥˜**: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ë¥˜, ë¶ˆëŸ‰ ê²€ì¶œ, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
- **ì±—ë´‡**: ëŒ€í™”í˜• ì¸í„°íŽ˜ì´ìŠ¤, ìžë™ ì‘ë‹µ, Q&A
- **ì—ì´ì „íŠ¸**: ìžìœ¨ ì˜ì‚¬ê²°ì •, ë³µìž¡í•œ ìž‘ì—… ìžë™í™”, ì›Œí¬í”Œë¡œìš° ìžë™í™”
- **ìµœì í™”**: ìžì› ìµœì í™”, ìŠ¤ì¼€ì¤„ë§, ê²½ë¡œ ìµœì í™”
- **ê°•í™”í•™ìŠµ**: í•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ê²°ì •, ì‹œë®¬ë ˆì´ì…˜ ìµœì í™”

### 2. ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼
{department_info} ì¡°ì§ ê´€ì ì—ì„œ ì´ ê³¼ì œì˜ ê²½ì˜íš¨ê³¼ë¥¼ ìš”ì•½í•˜ì„¸ìš” (2-3ë¬¸ìž¥):
- ì§€ì›ì„œì— ìž‘ì„±ëœ ê¸°ëŒ€íš¨ê³¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ìž‘ì„±
- ì¶”ì¸¡ì´ë‚˜ ê³¼ìž¥ ê¸ˆì§€

### 3. AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±
ì§€ì›ì„œ ë‚´ìš©(ì°¸ì—¬ì¸ì›, ê¸°ìˆ ì—­ëŸ‰, ë°ì´í„° ë“±)ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥ì„± í‰ê°€ (2-3ë¬¸ìž¥):
- ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œ ì°¸ê³ 
- ê¸°ìˆ ì  ë‚œì´ë„, ë°ì´í„° í™•ë³´, íŒ€ ì—­ëŸ‰ ë“±ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€

### 4. ì „ì²´ ì§€ì›ì„œ 5ì¤„ ìš”ì•½
ì´ ì§€ì›ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 5ì¤„ë¡œ ìš”ì•½:
1. ê³¼ì œ ëª©ì  (1ì¤„)
2. í˜„ìž¬ ë¬¸ì œ (1ì¤„)
3. í•´ê²° ë°©ì•ˆ (1ì¤„)
4. ê¸°ëŒ€ íš¨ê³¼ (1ì¤„)
5. êµ¬í˜„ ê³„íš (1ì¤„)

### 5. í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ë° ê·¼ê±° (5ì  ì²™ë„)
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì§€ì›ì„œë¥¼ í‰ê°€í•˜ê³ , ê° ê¸°ì¤€ë§ˆë‹¤ 1-5ì ê³¼ 2-3ë¬¸ìž¥ì˜ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”:

{self._build_criteria_guide(criteria_list)}
"""
        
        prompt = f"""{system_prompt}

{app_info}

---

## ì‘ë‹µ í˜•ì‹ (JSON)
**CRITICAL**: ë°˜ë“œì‹œ ì•„ëž˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

```json
{{
  "ai_category": "ì˜ˆì¸¡",
  "business_impact": "ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼ë¥¼ 2-3ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½",
  "technical_feasibility": "AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±ì„ 2-3ë¬¸ìž¥ìœ¼ë¡œ í‰ê°€",
  "five_line_summary": [
    "1. ê³¼ì œ ëª©ì ",
    "2. í˜„ìž¬ ë¬¸ì œ",
    "3. í•´ê²° ë°©ì•ˆ",
    "4. ê¸°ëŒ€ íš¨ê³¼",
    "5. êµ¬í˜„ ê³„íš"
  ],
  "evaluation_scores": {{
{self._build_json_format_example(criteria_list)}
  }}
}}
```

**ì¤‘ìš” ê·œì¹™:**
1. **ìœ íš¨í•œ JSON í˜•ì‹ í•„ìˆ˜** - ëª¨ë“  ë¬¸ìžì—´ì€ í°ë”°ì˜´í‘œ(")ë¡œ ê°ì‹¸ê¸°
2. **ai_categoryëŠ” ì •í™•ížˆ í•˜ë‚˜**: "ì˜ˆì¸¡", "ë¶„ë¥˜", "ì±—ë´‡", "ì—ì´ì „íŠ¸", "ìµœì í™”", "ê°•í™”í•™ìŠµ" ì¤‘ ì„ íƒ
3. **evaluation_scoresì˜ ê° scoreëŠ” 1-5 ì‚¬ì´ì˜ ì •ìˆ˜**
4. **ëª¨ë“  rationaleì€ ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œ ì‚¬ìš©** (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
5. **JSON ë‚´ë¶€ì—ì„œ ì¤„ë°”ê¿ˆì´ í•„ìš”í•˜ë©´ \\n ì‚¬ìš©**
6. **ë§ˆì§€ë§‰ í•­ëª© ë’¤ì—ëŠ” ì‰¼í‘œ(,) ì—†ìŒ** - JSON ë¬¸ë²• ì¤€ìˆ˜ í•„ìˆ˜
7. **ì¤‘ê´„í˜¸ì™€ ëŒ€ê´„í˜¸ë¥¼ ì •í™•ížˆ ë‹«ì„ ê²ƒ**
8. {department_info} ì¡°ì§ íŠ¹ì„± ë°˜ì˜

**ì‘ë‹µì€ JSONë§Œ í¬í•¨í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”.**
"""
        return prompt

    def _extract_json_from_text(self, text: str) -> Optional[str]:
        """
        Extract JSON from LLM response text using multiple strategies

        Args:
            text: Raw LLM response text

        Returns:
            Extracted JSON string or None
        """
        # Strategy 1: Remove markdown code blocks
        if "```json" in text:
            parts = text.split("```json")
            if len(parts) > 1:
                json_part = parts[1].split("```")[0]
                return json_part.strip()
        elif "```" in text:
            parts = text.split("```")
            if len(parts) > 1:
                json_part = parts[1]
                return json_part.strip()

        # Strategy 2: Find JSON object using regex (ì°¾ì•„ì„œ { } ë¸”ë¡ ì¶”ì¶œ)
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.finditer(json_pattern, text, re.DOTALL)
        for match in matches:
            json_candidate = match.group(0)
            try:
                # Validate it's valid JSON
                json.loads(json_candidate)
                return json_candidate
            except:
                continue

        # Strategy 3: Find first { to last }
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            json_candidate = text[start_idx:end_idx+1]
            return json_candidate

        # Strategy 4: Return original text (last resort)
        return text.strip()

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type(Exception)
    )
    def evaluate_with_single_llm(self, llm, prompt: str, llm_name: str = "LLM") -> Dict[str, Any]:
        """
        Evaluate application using a single LLM with robust JSON parsing

        Args:
            llm: LLM instance to use
            prompt: Evaluation prompt
            llm_name: Name of LLM for logging

        Returns:
            Evaluation result dictionary

        Raises:
            Exception: If evaluation fails after retries
        """
        # Apply rate limiting before LLM call
        self.rate_limiter.wait_if_needed()

        response = llm.invoke(prompt)
        content = response.content

        # Extract JSON from response
        json_text = self._extract_json_from_text(content)

        # Parse JSON
        try:
            result = json.loads(json_text)
            print(f"âœ… {llm_name} JSON parsed successfully")
            return result
        except json.JSONDecodeError as e:
            print(f"âŒ {llm_name} JSON parsing error: {e}")
            print(f"ðŸ“„ Response content (first 500 chars): {content[:500]}")
            print(f"ðŸ“„ Extracted JSON (first 500 chars): {json_text[:500]}")
            raise

    def evaluate_with_llm(self, prompt: str) -> Tuple[Dict[str, Any], Optional[Dict[str, Any]]]:
        """
        Evaluate application using LLM(s) with retry logic
        Returns results from both LLMs if ensemble mode is enabled

        Args:
            prompt: Evaluation prompt

        Returns:
            Tuple of (primary_result, secondary_result or None)

        Raises:
            Exception: If evaluation fails after retries
        """
        # Evaluate with primary LLM (A)
        result_a = self.evaluate_with_single_llm(self.llm_a, prompt, "LLM A")

        # Evaluate with secondary LLM (B) if available
        result_b = None
        if self.llm_b:
            try:
                result_b = self.evaluate_with_single_llm(self.llm_b, prompt, "LLM B")
            except Exception as e:
                print(f"âš ï¸  LLM B evaluation failed: {e}")
                print(f"â„¹ï¸  Continuing with LLM A result only")

        return result_a, result_b
    
    def _ensemble_results(self, result_a: Dict[str, Any], result_b: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ensemble results from two LLMs by averaging scores

        Args:
            result_a: Result from LLM A
            result_b: Result from LLM B

        Returns:
            Ensembled result dictionary
        """
        ensembled = {
            "ai_category": result_a.get("ai_category", "ë¶„ë¥˜"),  # Use A's category
            "business_impact": result_a.get("business_impact", ""),  # Use A's impact
            "technical_feasibility": result_a.get("technical_feasibility", ""),  # Use A's feasibility
            "five_line_summary": result_a.get("five_line_summary", []),  # Use A's summary
            "evaluation_scores": {}
        }

        # Ensemble evaluation scores by averaging
        scores_a = result_a.get("evaluation_scores", {})
        scores_b = result_b.get("evaluation_scores", {})

        # Get all criteria keys from both results
        all_criteria = set(scores_a.keys()) | set(scores_b.keys())

        for criterion in all_criteria:
            score_a_obj = scores_a.get(criterion, {})
            score_b_obj = scores_b.get(criterion, {})

            score_a = score_a_obj.get("score", 0) if isinstance(score_a_obj, dict) else 0
            score_b = score_b_obj.get("score", 0) if isinstance(score_b_obj, dict) else 0

            rationale_a = score_a_obj.get("rationale", "") if isinstance(score_a_obj, dict) else ""
            rationale_b = score_b_obj.get("rationale", "") if isinstance(score_b_obj, dict) else ""

            # Average the scores (round to nearest integer)
            if score_a > 0 and score_b > 0:
                avg_score = round((score_a + score_b) / 2)
                combined_rationale = f"[LLM A] {rationale_a}\n\n[LLM B] {rationale_b}"
            elif score_a > 0:
                avg_score = score_a
                combined_rationale = rationale_a
            elif score_b > 0:
                avg_score = score_b
                combined_rationale = rationale_b
            else:
                avg_score = 3  # Default to middle score
                combined_rationale = "í‰ê°€ ì ìˆ˜ë¥¼ ì‚°ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            ensembled["evaluation_scores"][criterion] = {
                "score": avg_score,
                "rationale": combined_rationale,
                "score_a": score_a,
                "score_b": score_b
            }

        print(f"âœ… Ensembled results from LLM A and LLM B")
        return ensembled

    def calculate_overall_grade(self, evaluation_detail: Dict[str, Any]) -> str:
        """
        Calculate overall grade from evaluation details
        
        Args:
            evaluation_detail: Dictionary of evaluation scores
            
        Returns:
            Overall grade (S/A/B/C/D)
        """
        total_score = 0
        count = 0
        
        for item in evaluation_detail.values():
            if isinstance(item, dict) and "score" in item:
                total_score += item["score"]
                count += 1
        
        if count == 0:
            return "C"
        
        avg_score = total_score / count
        
        if avg_score >= 4.5:
            return "S"
        elif avg_score >= 3.5:
            return "A"
        elif avg_score >= 2.5:
            return "B"
        elif avg_score >= 1.5:
            return "C"
        else:
            return "D"
    
    def evaluate_application(
        self, 
        db: Session, 
        application: Application,
        criteria_list: Optional[List[EvaluationCriteria]] = None
    ) -> bool:
        """
        Evaluate single application
        
        Args:
            db: Database session
            application: Application to evaluate
            criteria_list: Evaluation criteria (optional, will fetch if None)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get evaluation criteria if not provided (backward compatibility)
            if criteria_list is None:
                criteria_list = db.query(EvaluationCriteria).filter(
                    EvaluationCriteria.is_active == True
                ).order_by(EvaluationCriteria.display_order).all()
            
            # Build prompt
            prompt = self.build_evaluation_prompt(application, criteria_list or [])

            # Evaluate with LLM(s)
            print(f"ðŸ¤– Evaluating application {application.id} ({application.subject})...")
            result_a, result_b = self.evaluate_with_llm(prompt)

            # Ensemble results if both LLMs returned results
            if result_b:
                print(f"ðŸ”„ Ensembling results from LLM A and LLM B...")
                result = self._ensemble_results(result_a, result_b)
            else:
                result = result_a

            # Extract results
            ai_category = result.get("ai_category", "ë¶„ë¥˜")
            business_impact = result.get("business_impact", "")
            technical_feasibility = result.get("technical_feasibility", "")
            five_line_summary = result.get("five_line_summary", [])
            evaluation_scores = result.get("evaluation_scores", {})

            # Build AI categories for compatibility
            ai_categories = [{
                "category": ai_category,
                "description": "ì§€ì›ì„œ ê¸°ë°˜ AI ìš”ì•½"
            }]

            # Build evaluation detail with scores
            evaluation_detail = {
                "ai_category": ai_category,
                "business_impact": business_impact,
                "technical_feasibility": technical_feasibility,
                "five_line_summary": five_line_summary,
                "evaluation_scores": evaluation_scores
            }

            # Calculate overall grade from evaluation scores
            if evaluation_scores:
                scores = []
                for criterion in ["innovation", "feasibility", "impact", "clarity"]:
                    if criterion in evaluation_scores and "score" in evaluation_scores[criterion]:
                        scores.append(evaluation_scores[criterion]["score"])

                if scores:
                    avg_score = sum(scores) / len(scores)
                    # Convert average to grade (S/A/B/C/D)
                    if avg_score >= 4.5:
                        overall_grade = "S"
                    elif avg_score >= 3.5:
                        overall_grade = "A"
                    elif avg_score >= 2.5:
                        overall_grade = "B"
                    elif avg_score >= 1.5:
                        overall_grade = "C"
                    else:
                        overall_grade = "D"
                else:
                    overall_grade = "B"  # Default
            else:
                # Fallback to old simple logic if scores not provided
                if "ì–´ë µ" in technical_feasibility or "ë¶ˆê°€ëŠ¥" in technical_feasibility:
                    overall_grade = "C"
                elif "ê°€ëŠ¥" in technical_feasibility and "ì¶©ë¶„" in technical_feasibility:
                    overall_grade = "A"
                else:
                    overall_grade = "B"
            
            # Build summary
            summary_parts = []
            summary_parts.append(f"**AI ê¸°ìˆ  ë¶„ë¥˜**: {ai_category}\n\n")
            summary_parts.append(f"**ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼**\n{business_impact}\n\n")
            summary_parts.append(f"**AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±**\n{technical_feasibility}\n\n")
            summary_parts.append(f"**ì „ì²´ ì§€ì›ì„œ 5ì¤„ ìš”ì•½**\n" + "\n".join(five_line_summary))
            
            summary = "".join(summary_parts)
            
            # Update application
            application.ai_categories = ai_categories
            application.ai_category_primary = ai_category
            application.ai_evaluation_detail = evaluation_detail
            application.ai_grade = overall_grade
            application.ai_summary = summary
            application.ai_evaluated_at = datetime.utcnow()
            application.status = "ai_evaluated"
            
            # Save evaluation history
            history = EvaluationHistory(
                application_id=application.id,
                evaluator_id=None,
                evaluator_type="AI",
                grade=overall_grade,
                summary=summary,
                evaluation_detail=evaluation_detail,
                ai_categories=ai_categories
            )
            db.add(history)
            
            db.commit()
            print(f"âœ… Application {application.id} evaluated: {overall_grade} ({ai_category})")
            return True
            
        except Exception as e:
            print(f"âŒ Error evaluating application {application.id}: {e}")
            import traceback
            traceback.print_exc()
            db.rollback()
            return False
    
    def _score_to_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 4.5:
            return "S"
        elif score >= 3.5:
            return "A"
        elif score >= 2.5:
            return "B"
        elif score >= 1.5:
            return "C"
        else:
            return "D"


# Singleton instance
llm_evaluator = LLMEvaluator()

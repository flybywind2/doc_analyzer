"""
LLM Evaluator Service
"""
import re
import uuid
import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from app.config import settings
from app.models.application import Application
from app.models.evaluation import EvaluationCriteria, EvaluationHistory
from app.services.rate_limiter import RateLimiter


class LLMEvaluator:
    """LLM-based application evaluator with ensemble support"""

    def __init__(self):
        # Primary LLM (A)
        self.llm_a = ChatOpenAI(
            base_url=settings.llm_api_base_url,
            api_key=settings.llm_api_key,
            model=settings.llm_model_name,
            temperature=0.1,  # ì¼ê´€ì„± í™•ë³´
            default_headers={
                "x-dep-ticket": settings.llm_credential_key,
                "Send-System-Name": settings.llm_system_name,
                "User-ID": settings.llm_user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )

        # Secondary LLM (B) - Optional for ensemble
        self.llm_b = None
        if settings.llm_b_api_base_url and settings.llm_b_api_key:
            self.llm_b = ChatOpenAI(
                base_url=settings.llm_b_api_base_url,
                api_key=settings.llm_b_api_key,
                model=settings.llm_b_model_name or settings.llm_model_name,
                temperature=0.1,
                default_headers={
                    "x-dep-ticket": settings.llm_b_credential_key or settings.llm_credential_key,
                    "Send-System-Name": settings.llm_system_name,
                    "User-ID": settings.llm_user_id,
                    "User-Type": "AD",
                    "Prompt-Msg-Id": str(uuid.uuid4()),
                    "Completion-Msg-Id": str(uuid.uuid4()),
                },
            )
            print(f"âœ… Ensemble mode enabled: LLM A ({settings.llm_model_name}) + LLM B ({settings.llm_b_model_name or settings.llm_model_name})")
        else:
            print(f"â„¹ï¸  Single LLM mode: {settings.llm_model_name}")

        # Rate limiter: 20 calls per minute
        self.rate_limiter = RateLimiter(max_calls=20, time_window=60)

        # Criteria name to key mapping (í•œê¸€ -> ì˜ë¬¸)
        self.criteria_key_map = {
            "í˜ì‹ ì„±": "innovation",
            "ì‹¤í˜„ê°€ëŠ¥ì„±": "feasibility",
            "íš¨ê³¼ì„±": "impact",
            "ëª…í™•ì„±": "clarity"
        }

    def _build_criteria_guide(self, criteria_list: List[EvaluationCriteria]) -> str:
        """
        Build evaluation criteria guide from database criteria

        Args:
            criteria_list: List of evaluation criteria from DB

        Returns:
            Formatted criteria guide string
        """
        if not criteria_list:
            # Fallback to default if no criteria
            return """
**í˜ì‹ ì„± (Innovation)**: AI ê¸°ìˆ ì˜ ì°½ì˜ì„±ê³¼ ìƒˆë¡œì›€ (1-5ì )
**ì‹¤í˜„ê°€ëŠ¥ì„± (Feasibility)**: ê¸°ìˆ ì  êµ¬í˜„ ë‚œì´ë„ì™€ íŒ€ ì—­ëŸ‰ (1-5ì )
**íš¨ê³¼ì„± (Impact)**: ì¡°ì§ì— ë¯¸ì¹˜ëŠ” ê²½ì˜ íš¨ê³¼ (1-5ì )
**ëª…í™•ì„± (Clarity)**: ë¬¸ì œ ì •ì˜ì™€ í•´ê²° ë°©ì•ˆì˜ êµ¬ì²´ì„± (1-5ì )
""".strip()

        guide_parts = []
        for criteria in criteria_list:
            key = self.criteria_key_map.get(criteria.name, criteria.name.lower())
            guide_parts.append(f"""
**{criteria.name} ({key.capitalize()})**: {criteria.description}

{criteria.evaluation_guide}
""".strip())

        return "\n\n".join(guide_parts)

    def _build_json_format_example(self, criteria_list: List[EvaluationCriteria]) -> str:
        """
        Build JSON format example from database criteria

        Args:
            criteria_list: List of evaluation criteria from DB

        Returns:
            Formatted JSON example string
        """
        if not criteria_list:
            # Fallback to default
            criteria_list = [
                type('obj', (object,), {'name': 'í˜ì‹ ì„±', 'description': 'AI ê¸°ìˆ ì˜ ì°½ì˜ì„±ê³¼ ìƒˆë¡œì›€'})(),
                type('obj', (object,), {'name': 'ì‹¤í˜„ê°€ëŠ¥ì„±', 'description': 'ê¸°ìˆ ì  êµ¬í˜„ ë‚œì´ë„ì™€ íŒ€ ì—­ëŸ‰'})(),
                type('obj', (object,), {'name': 'íš¨ê³¼ì„±', 'description': 'ì¡°ì§ì— ë¯¸ì¹˜ëŠ” ê²½ì˜ íš¨ê³¼'})(),
                type('obj', (object,), {'name': 'ëª…í™•ì„±', 'description': 'ë¬¸ì œ ì •ì˜ì™€ í•´ê²° ë°©ì•ˆì˜ êµ¬ì²´ì„±'})()
            ]

        json_parts = []
        for criteria in criteria_list:
            key = self.criteria_key_map.get(criteria.name, criteria.name.lower())
            json_parts.append(f'''    "{key}": {{
      "score": 1-5 ì‚¬ì´ì˜ ì •ìˆ˜,
      "rationale": "{criteria.name} í‰ê°€ ê·¼ê±° (2-3ë¬¸ìž¥, ì§€ì›ì„œ ê¸°ë°˜)"
    }}''')

        return ",\n".join(json_parts)

    def build_evaluation_prompt(
        self, 
        application: Application, 
        criteria_list: List[EvaluationCriteria]
    ) -> str:
        """
        Build evaluation prompt for LLM
        
        Args:
            application: Application to evaluate
            criteria_list: List of evaluation criteria
            
        Returns:
            Formatted prompt string
        """
        # ê³¼ì œ ì •ë³´ êµ¬ì„±
        department_info = f"{application.division or 'N/A'} > {application.department.name if application.department else 'N/A'}"
        
        system_prompt = f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ëŒ€ê¸°ì—…ì˜ AI ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì¡°ì§: {department_info}

ì—­í• : ì§€ì›ì„œ ë‚´ìš©ì„ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
2. {department_info} ì¡°ì§ì˜ ì—…ë¬´ íŠ¹ì„±ì„ ê³ ë ¤í•œ í•´ì„
3. ì‚¬ì‹¤ ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
4. ê³¼ìž¥í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
"""

        app_info = f"""
# AI ê³¼ì œ ì§€ì›ì„œ í‰ê°€

## ê³¼ì œ ê¸°ë³¸ ì •ë³´
- ê³¼ì œëª…: {application.subject or 'N/A'}
- ì¡°ì§: {department_info}
- ì°¸ì—¬ ì¸ì›: {application.participant_count or 'N/A'}ëª…
- ëŒ€í‘œìž: {application.representative_name or 'N/A'}

## ì‹ ì²­ ë‚´ìš©
### í˜„ìž¬ ì—…ë¬´
{application.current_work or 'N/A'}

### Pain Point (í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œ)
{application.pain_point or 'N/A'}

### ê°œì„  ì•„ì´ë””ì–´
{application.improvement_idea or 'N/A'}

### ê¸°ëŒ€ íš¨ê³¼
{application.expected_effect or 'N/A'}

### ë°”ë¼ëŠ” ì 
{application.hope or 'N/A'}

## ì‚¬ì „ ì„¤ë¬¸
{json.dumps(application.pre_survey, ensure_ascii=False, indent=2) if application.pre_survey else 'N/A'}

## ì°¸ì—¬ìž ê¸°ìˆ  ì—­ëŸ‰
{json.dumps(application.tech_capabilities, ensure_ascii=False, indent=2) if application.tech_capabilities else 'N/A'}

---

## í‰ê°€ ìš”ì²­ì‚¬í•­

ì§€ì›ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìš”ì•½í•˜ê³  í‰ê°€í•˜ì„¸ìš”:

### 1. AI ê¸°ìˆ  ë¶„ë¥˜
ì§€ì›ì„œì—ì„œ ì–¸ê¸‰ëœ AI ê¸°ìˆ ì„ ë‹¤ìŒ ì¤‘ **í•˜ë‚˜ë§Œ** ì„ íƒí•˜ì„¸ìš”:
- **ì˜ˆì¸¡**: ë¯¸ëž˜ ê°’ ì˜ˆì¸¡, ìˆ˜ìš” ì˜ˆì¸¡, íŠ¸ë Œë“œ ë¶„ì„
- **ë¶„ë¥˜**: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ë¥˜, ë¶ˆëŸ‰ ê²€ì¶œ, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
- **ì±—ë´‡**: ëŒ€í™”í˜• ì¸í„°íŽ˜ì´ìŠ¤, ìžë™ ì‘ë‹µ, Q&A
- **ì—ì´ì „íŠ¸**: ìžìœ¨ ì˜ì‚¬ê²°ì •, ë³µìž¡í•œ ìž‘ì—… ìžë™í™”, ì›Œí¬í”Œë¡œìš° ìžë™í™”
- **ìµœì í™”**: ìžì› ìµœì í™”, ìŠ¤ì¼€ì¤„ë§, ê²½ë¡œ ìµœì í™”
- **ê°•í™”í•™ìŠµ**: í•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ê²°ì •, ì‹œë®¬ë ˆì´ì…˜ ìµœì í™”

### 2. ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼
{department_info} ì¡°ì§ ê´€ì ì—ì„œ ì´ ê³¼ì œì˜ ê²½ì˜íš¨ê³¼ë¥¼ ìš”ì•½í•˜ì„¸ìš” (2-3ë¬¸ìž¥):
- ì§€ì›ì„œì— ìž‘ì„±ëœ ê¸°ëŒ€íš¨ê³¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ìž‘ì„±
- ì¶”ì¸¡ì´ë‚˜ ê³¼ìž¥ ê¸ˆì§€

### 3. AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±
ì§€ì›ì„œ ë‚´ìš©(ì°¸ì—¬ì¸ì›, ê¸°ìˆ ì—­ëŸ‰, ë°ì´í„° ë“±)ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥ì„± í‰ê°€ (2-3ë¬¸ìž¥):
- ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œ ì°¸ê³ 
- ê¸°ìˆ ì  ë‚œì´ë„, ë°ì´í„° í™•ë³´, íŒ€ ì—­ëŸ‰ ë“±ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€

### 4. ì „ì²´ ì§€ì›ì„œ 5ì¤„ ìš”ì•½
ì´ ì§€ì›ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 5ì¤„ë¡œ ìš”ì•½:
1. ê³¼ì œ ëª©ì  (1ì¤„)
2. í˜„ìž¬ ë¬¸ì œ (1ì¤„)
3. í•´ê²° ë°©ì•ˆ (1ì¤„)
4. ê¸°ëŒ€ íš¨ê³¼ (1ì¤„)
5. êµ¬í˜„ ê³„íš (1ì¤„)

### 5. í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ë° ê·¼ê±° (5ì  ì²™ë„)
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì§€ì›ì„œë¥¼ í‰ê°€í•˜ê³ , ê° ê¸°ì¤€ë§ˆë‹¤ 1-5ì ê³¼ 2-3ë¬¸ìž¥ì˜ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”:

{self._build_criteria_guide(criteria_list)}
"""
        
        prompt = f"""{system_prompt}

{app_info}

---

## ì‘ë‹µ í˜•ì‹ (JSON)
**CRITICAL**: ë°˜ë“œì‹œ ì•„ëž˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

```json
{{
  "ai_category": "ì˜ˆì¸¡",
  "business_impact": "ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼ë¥¼ 2-3ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½",
  "technical_feasibility": "AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±ì„ 2-3ë¬¸ìž¥ìœ¼ë¡œ í‰ê°€",
  "five_line_summary": [
    "1. ê³¼ì œ ëª©ì ",
    "2. í˜„ìž¬ ë¬¸ì œ",
    "3. í•´ê²° ë°©ì•ˆ",
    "4. ê¸°ëŒ€ íš¨ê³¼",
    "5. êµ¬í˜„ ê³„íš"
  ],
  "evaluation_scores": {{
{self._build_json_format_example(criteria_list)}
  }}
}}
```

**ì¤‘ìš” ê·œì¹™:**
1. **ìœ íš¨í•œ JSON í˜•ì‹ í•„ìˆ˜** - ëª¨ë“  ë¬¸ìžì—´ì€ í°ë”°ì˜´í‘œ(")ë¡œ ê°ì‹¸ê¸°
2. **ai_categoryëŠ” ì •í™•ížˆ í•˜ë‚˜**: "ì˜ˆì¸¡", "ë¶„ë¥˜", "ì±—ë´‡", "ì—ì´ì „íŠ¸", "ìµœì í™”", "ê°•í™”í•™ìŠµ" ì¤‘ ì„ íƒ
3. **evaluation_scoresì˜ ê° scoreëŠ” 1-5 ì‚¬ì´ì˜ ì •ìˆ˜**
4. **ëª¨ë“  rationaleì€ ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œ ì‚¬ìš©** (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
5. **JSON ë‚´ë¶€ì—ì„œ ì¤„ë°”ê¿ˆì´ í•„ìš”í•˜ë©´ \\n ì‚¬ìš©**
6. **ë§ˆì§€ë§‰ í•­ëª© ë’¤ì—ëŠ” ì‰¼í‘œ(,) ì—†ìŒ** - JSON ë¬¸ë²• ì¤€ìˆ˜ í•„ìˆ˜
7. **ì¤‘ê´„í˜¸ì™€ ëŒ€ê´„í˜¸ë¥¼ ì •í™•ížˆ ë‹«ì„ ê²ƒ**
8. {department_info} ì¡°ì§ íŠ¹ì„± ë°˜ì˜

**ì‘ë‹µì€ JSONë§Œ í¬í•¨í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”.**
"""
        return prompt

    def _extract_json_from_text(self, text: str) -> Optional[str]:
        """
        Extract JSON from LLM response text using multiple strategies

        Args:
            text: Raw LLM response text

        Returns:
            Extracted JSON string or None
        """
        # Strategy 1: Remove markdown code blocks
        if "```json" in text:
            parts = text.split("```json")
            if len(parts) > 1:
                json_part = parts[1].split("```")[0]
                return json_part.strip()
        elif "```" in text:
            parts = text.split("```")
            if len(parts) > 1:
                json_part = parts[1]
                return json_part.strip()

        # Strategy 2: Find JSON object using regex (ì°¾ì•„ì„œ { } ë¸”ë¡ ì¶”ì¶œ)
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.finditer(json_pattern, text, re.DOTALL)
        for match in matches:
            json_candidate = match.group(0)
            try:
                # Validate it's valid JSON
                json.loads(json_candidate)
                return json_candidate
            except:
                continue

        # Strategy 3: Find first { to last }
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            json_candidate = text[start_idx:end_idx+1]
            return json_candidate

        # Strategy 4: Return original text (last resort)
        return text.strip()

    def _print_prompt(self, llm_name: str, prompt: str, step: str = ""):
        """Print prompt to terminal for debugging"""
        separator = "=" * 80
        print(f"\n{separator}")
        print(f"ðŸŽ¯ {llm_name} PROMPT {step}")
        print(f"{separator}")
        print(prompt)
        print(f"{separator}\n")

    def _print_response(self, llm_name: str, content: str, step: str = ""):
        """Print response to terminal for debugging"""
        separator = "=" * 80
        print(f"\n{separator}")
        print(f"ðŸ’¬ {llm_name} RESPONSE {step}")
        print(f"{separator}")
        print(content)
        print(f"{separator}\n")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type(Exception)
    )
    def evaluate_with_single_llm(self, llm, prompt: str, llm_name: str = "LLM", step: str = "", verbose: bool = True) -> Dict[str, Any]:
        """
        Evaluate application using a single LLM with robust JSON parsing

        Args:
            llm: LLM instance to use
            prompt: Evaluation prompt
            llm_name: Name of LLM for logging
            step: Step description for logging
            verbose: Whether to print prompts and responses

        Returns:
            Evaluation result dictionary

        Raises:
            Exception: If evaluation fails after retries
        """
        # Print prompt if verbose
        if verbose:
            self._print_prompt(llm_name, prompt, step)

        # Apply rate limiting before LLM call
        self.rate_limiter.wait_if_needed()

        response = llm.invoke(prompt)
        content = response.content

        # Print response if verbose
        if verbose:
            self._print_response(llm_name, content, step)

        # Extract JSON from response
        json_text = self._extract_json_from_text(content)

        # Parse JSON
        try:
            result = json.loads(json_text)
            print(f"âœ… {llm_name} JSON parsed successfully")
            return result
        except json.JSONDecodeError as e:
            print(f"âŒ {llm_name} JSON parsing error: {e}")
            print(f"ðŸ“„ Response content (first 500 chars): {content[:500]}")
            print(f"ðŸ“„ Extracted JSON (first 500 chars): {json_text[:500]}")
            raise

    def build_debate_prompt(
        self,
        application: Application,
        criteria_list: List[EvaluationCriteria],
        llm_a_result: Dict[str, Any]
    ) -> str:
        """
        Build debate prompt for LLM B to review and refine LLM A's evaluation

        Args:
            application: Application to evaluate
            criteria_list: List of evaluation criteria
            llm_a_result: LLM A's evaluation result

        Returns:
            Formatted debate prompt string
        """
        department_info = f"{application.division or 'N/A'} > {application.department.name if application.department else 'N/A'}"

        system_prompt = f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ëŒ€ê¸°ì—…ì˜ AI ì „ë¬¸ê°€ì´ìž í‰ê°€ ê²€í† ìžìž…ë‹ˆë‹¤.
ì¡°ì§: {department_info}

ì—­í• : ë™ë£Œ AI ì „ë¬¸ê°€(LLM A)ì˜ í‰ê°€ë¥¼ ê²€í† í•˜ê³ , ë” ë‚˜ì€ í‰ê°€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. LLM Aì˜ í‰ê°€ë¥¼ ì¡´ì¤‘í•˜ë˜, ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ì€ ìˆ˜ì •
2. ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
3. {department_info} ì¡°ì§ì˜ ì—…ë¬´ íŠ¹ì„±ì„ ê³ ë ¤
4. ì ìˆ˜ëŠ” ê³¼ìž¥í•˜ê±°ë‚˜ ë‚®ì¶”ì§€ ë§ê³  ê°ê´€ì ìœ¼ë¡œ í‰ê°€
5. LLM Aì™€ ì˜ê²¬ì´ ë‹¤ë¥´ë©´ ê·¼ê±°ë¥¼ ëª…í™•ížˆ ì œì‹œ
"""

        llm_a_summary = json.dumps(llm_a_result, ensure_ascii=False, indent=2)

        debate_prompt = f"""{system_prompt}

---

## ì§€ì›ì„œ ì •ë³´

ê³¼ì œëª…: {application.subject or 'N/A'}
ì¡°ì§: {department_info}
ì°¸ì—¬ ì¸ì›: {application.participant_count or 'N/A'}ëª…

### Pain Point
{application.pain_point or 'N/A'}

### ê°œì„  ì•„ì´ë””ì–´
{application.improvement_idea or 'N/A'}

### ê¸°ëŒ€ íš¨ê³¼
{application.expected_effect or 'N/A'}

---

## LLM Aì˜ í‰ê°€ ê²°ê³¼

```json
{llm_a_summary}
```

---

## ìš”ì²­ì‚¬í•­

ìœ„ ì§€ì›ì„œì™€ LLM Aì˜ í‰ê°€ë¥¼ ê²€í† í•˜ì—¬, **ë” ë‚˜ì€ í‰ê°€**ë¥¼ ì œì‹œí•˜ì„¸ìš”.

### ê²€í†  ì§€ì¹¨

1. **AI ê¸°ìˆ  ë¶„ë¥˜**: LLM Aì˜ ì„ íƒì´ ì ì ˆí•œê°€? ì§€ì›ì„œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?

2. **í‰ê°€ ì ìˆ˜**: ê° ê¸°ì¤€ë³„ ì ìˆ˜ê°€ ì§€ì›ì„œ ë‚´ìš©ì„ ì •í™•ížˆ ë°˜ì˜í•˜ëŠ”ê°€?
   - ë„ˆë¬´ ê´€ëŒ€í•˜ê±°ë‚˜ ì—„ê²©í•˜ì§€ ì•Šì€ê°€?
   - ê·¼ê±°ê°€ ëª…í™•í•œê°€?

3. **ê°œì„ ì **:
   - LLM Aê°€ ë†“ì¹œ ì¤‘ìš”í•œ ë‚´ìš©ì€?
   - ê³¼ìž¥ë˜ê±°ë‚˜ ê³¼ì†Œí‰ê°€ëœ ë¶€ë¶„ì€?
   - ë” êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•  ìˆ˜ ìžˆëŠ”ê°€?

### ì‘ë‹µ í˜•ì‹ (JSON)

**CRITICAL**: ë°˜ë“œì‹œ ì•„ëž˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

```json
{{
  "ai_category": "ì˜ˆì¸¡",
  "business_impact": "ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼ë¥¼ 2-3ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½ (LLM A ê°œì„ )",
  "technical_feasibility": "AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±ì„ 2-3ë¬¸ìž¥ìœ¼ë¡œ í‰ê°€ (LLM A ê°œì„ )",
  "five_line_summary": [
    "1. ê³¼ì œ ëª©ì ",
    "2. í˜„ìž¬ ë¬¸ì œ",
    "3. í•´ê²° ë°©ì•ˆ",
    "4. ê¸°ëŒ€ íš¨ê³¼",
    "5. êµ¬í˜„ ê³„íš"
  ],
  "evaluation_scores": {{
{self._build_json_format_example(criteria_list)}
  }},
  "debate_summary": "LLM Aì˜ í‰ê°€ì™€ ë¹„êµí•˜ì—¬ ì–´ë–¤ ì ì„ ê°œì„ í–ˆëŠ”ì§€ 2-3ë¬¸ìž¥ìœ¼ë¡œ ì„¤ëª…"
}}
```

**ì¤‘ìš” ê·œì¹™:**
1. **ìœ íš¨í•œ JSON í˜•ì‹ í•„ìˆ˜**
2. **ai_categoryëŠ” ì •í™•ížˆ í•˜ë‚˜**: "ì˜ˆì¸¡", "ë¶„ë¥˜", "ì±—ë´‡", "ì—ì´ì „íŠ¸", "ìµœì í™”", "ê°•í™”í•™ìŠµ" ì¤‘ ì„ íƒ
3. **evaluation_scoresì˜ ê° scoreëŠ” 1-5 ì‚¬ì´ì˜ ì •ìˆ˜**
4. **rationaleì€ ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œ ì‚¬ìš©** (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
5. **LLM Aì™€ ì ìˆ˜ê°€ ë‹¤ë¥´ë©´ debate_summaryì— ì´ìœ  ì„¤ëª…**
6. **JSON ë‚´ë¶€ì—ì„œ ì¤„ë°”ê¿ˆì´ í•„ìš”í•˜ë©´ \\n ì‚¬ìš©**
7. **ì‘ë‹µì€ JSONë§Œ í¬í•¨í•˜ì„¸ìš”**
"""
        return debate_prompt

    def build_final_evaluation_prompt(
        self,
        application: Application,
        criteria_list: List[EvaluationCriteria],
        llm_a_result: Dict[str, Any],
        llm_b_result: Dict[str, Any]
    ) -> str:
        """
        Build final evaluation prompt for LLM A to consider LLM B's review

        Args:
            application: Application to evaluate
            criteria_list: List of evaluation criteria
            llm_a_result: LLM A's initial evaluation
            llm_b_result: LLM B's review and refinement

        Returns:
            Formatted final evaluation prompt
        """
        department_info = f"{application.division or 'N/A'} > {application.department.name if application.department else 'N/A'}"

        system_prompt = f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ëŒ€ê¸°ì—…ì˜ AI ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì¡°ì§: {department_info}

ì—­í• : ë‹¹ì‹ ì˜ ì´ˆê¸° í‰ê°€ì™€ ë™ë£Œ í‰ê°€ìž(LLM B)ì˜ ê²€í† ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… í‰ê°€ë¥¼ ë‚´ë¦½ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. ë‹¹ì‹ ì˜ ì´ˆê¸° í‰ê°€ì™€ LLM Bì˜ ê²€í† ë¥¼ ëª¨ë‘ ê³ ë ¤
2. LLM Bì˜ ì§€ì ì´ íƒ€ë‹¹í•˜ë©´ ìˆ˜ìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·¼ê±°ë¥¼ ì œì‹œí•˜ë©° ì›ëž˜ í‰ê°€ ìœ ì§€
3. ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
4. ìµœì¢… í‰ê°€ëŠ” ê°€ìž¥ ê°ê´€ì ì´ê³  ê³µì •í•œ ê²°ê³¼ê°€ ë˜ì–´ì•¼ í•¨
5. {department_info} ì¡°ì§ì˜ ì—…ë¬´ íŠ¹ì„±ì„ ê³ ë ¤
"""

        llm_a_summary = json.dumps(llm_a_result, ensure_ascii=False, indent=2)
        llm_b_summary = json.dumps(llm_b_result, ensure_ascii=False, indent=2)

        final_prompt = f"""{system_prompt}

---

## ì§€ì›ì„œ ì •ë³´

ê³¼ì œëª…: {application.subject or 'N/A'}
ì¡°ì§: {department_info}

### Pain Point
{application.pain_point or 'N/A'}

### ê°œì„  ì•„ì´ë””ì–´
{application.improvement_idea or 'N/A'}

### ê¸°ëŒ€ íš¨ê³¼
{application.expected_effect or 'N/A'}

---

## í‰ê°€ ê³¼ì •

### 1ë‹¨ê³„: ë‹¹ì‹ ì˜ ì´ˆê¸° í‰ê°€ (LLM A)

```json
{llm_a_summary}
```

### 2ë‹¨ê³„: ë™ë£Œ í‰ê°€ìžì˜ ê²€í†  (LLM B)

```json
{llm_b_summary}
```

---

## ìµœì¢… í‰ê°€ ìš”ì²­

ìœ„ í‰ê°€ ê³¼ì •ì„ ê²€í† í•˜ì—¬ **ìµœì¢… í‰ê°€**ë¥¼ ë‚´ë ¤ì£¼ì„¸ìš”.

### ê²€í†  ì‚¬í•­

1. **LLM Bì˜ ì§€ì ì´ íƒ€ë‹¹í•œê°€?**
   - ì§€ì›ì„œ ë‚´ìš©ì„ ë” ì •í™•ížˆ ë°˜ì˜í–ˆëŠ”ê°€?
   - ë†“ì¹œ ì¤‘ìš”í•œ ë‚´ìš©ì„ ë°œê²¬í–ˆëŠ”ê°€?
   - ì ìˆ˜ ì¡°ì •ì´ í•©ë¦¬ì ì¸ê°€?

2. **ë‹¹ì‹ ì˜ ì´ˆê¸° í‰ê°€ë¥¼ ìœ ì§€í•  ë¶€ë¶„ì€?**
   - LLM Bê°€ ê³¼ìž¥í•˜ê±°ë‚˜ ìž˜ëª» í•´ì„í•œ ë¶€ë¶„ì€?
   - ì´ˆê¸° í‰ê°€ê°€ ë” ê°ê´€ì ì´ì—ˆë˜ ë¶€ë¶„ì€?

3. **ìµœì¢… íŒë‹¨**
   - ê° í‰ê°€ ê¸°ì¤€ë³„ë¡œ ìµœì¢… ì ìˆ˜ì™€ ê·¼ê±° ê²°ì •
   - ë‘ í‰ê°€ë¥¼ ì¢…í•©í•œ ê· í˜•ìž¡ížŒ ê²°ê³¼ ë„ì¶œ

### ì‘ë‹µ í˜•ì‹ (JSON)

**CRITICAL**: ë°˜ë“œì‹œ ì•„ëž˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

```json
{{
  "ai_category": "ì˜ˆì¸¡",
  "business_impact": "ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼ (ìµœì¢… íŒë‹¨)",
  "technical_feasibility": "AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„± (ìµœì¢… íŒë‹¨)",
  "five_line_summary": [
    "1. ê³¼ì œ ëª©ì ",
    "2. í˜„ìž¬ ë¬¸ì œ",
    "3. í•´ê²° ë°©ì•ˆ",
    "4. ê¸°ëŒ€ íš¨ê³¼",
    "5. êµ¬í˜„ ê³„íš"
  ],
  "evaluation_scores": {{
{self._build_json_format_example(criteria_list)}
  }},
  "final_decision": "ì´ˆê¸° í‰ê°€ì™€ ê²€í†  ì˜ê²¬ì„ ì¢…í•©í•œ ìµœì¢… íŒë‹¨ ê·¼ê±°ë¥¼ 2-3ë¬¸ìž¥ìœ¼ë¡œ ì„¤ëª…"
}}
```

**ì¤‘ìš” ê·œì¹™:**
1. **ìœ íš¨í•œ JSON í˜•ì‹ í•„ìˆ˜**
2. **ai_categoryëŠ” ì •í™•ížˆ í•˜ë‚˜**: "ì˜ˆì¸¡", "ë¶„ë¥˜", "ì±—ë´‡", "ì—ì´ì „íŠ¸", "ìµœì í™”", "ê°•í™”í•™ìŠµ" ì¤‘ ì„ íƒ
3. **evaluation_scoresì˜ ê° scoreëŠ” 1-5 ì‚¬ì´ì˜ ì •ìˆ˜**
4. **rationaleì€ ìµœì¢… íŒë‹¨ ê·¼ê±°ë¥¼ ëª…í™•ížˆ ìž‘ì„±**
5. **final_decisionì— ì´ˆê¸° í‰ê°€ì™€ ê²€í†  ì˜ê²¬ì„ ì–´ë–»ê²Œ ì¢…í•©í–ˆëŠ”ì§€ ì„¤ëª…**
6. **JSON ë‚´ë¶€ì—ì„œ ì¤„ë°”ê¿ˆì´ í•„ìš”í•˜ë©´ \\n ì‚¬ìš©**
7. **ì‘ë‹µì€ JSONë§Œ í¬í•¨í•˜ì„¸ìš”**
"""
        return final_prompt

    def evaluate_with_llm(self, prompt: str) -> Tuple[Dict[str, Any], Optional[Dict[str, Any]]]:
        """
        Evaluate application using LLM(s) with debate mode
        If both LLMs available: LLM A evaluates first, then LLM B reviews and refines

        Args:
            prompt: Evaluation prompt

        Returns:
            Tuple of (primary_result, secondary_result or None)

        Raises:
            Exception: If evaluation fails after retries
        """
        # Evaluate with primary LLM (A)
        result_a = self.evaluate_with_single_llm(self.llm_a, prompt, "LLM A")

        # If LLM B available, use debate mode
        result_b = None
        if self.llm_b:
            try:
                # LLM B reviews LLM A's evaluation
                print(f"ðŸ”„ Starting debate mode: LLM B reviewing LLM A's evaluation...")

                # Extract application and criteria from context (need to pass them)
                # For now, use the same prompt - will be improved in evaluate_application
                result_b = self.evaluate_with_single_llm(self.llm_b, prompt, "LLM B (Initial)")
                print(f"âœ… Debate mode: LLM B provided refined evaluation")
            except Exception as e:
                print(f"âš ï¸  LLM B evaluation failed: {e}")
                print(f"â„¹ï¸  Continuing with LLM A result only")

        return result_a, result_b

    def evaluate_with_multiturn_debate(
        self,
        application: Application,
        criteria_list: List[EvaluationCriteria]
    ) -> Tuple[Dict[str, Any], Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
        """
        Evaluate using 3-step debate mode with multiturn conversation for LLM A
        LLM A maintains conversation context across Step 1 and Step 3

        Args:
            application: Application to evaluate
            criteria_list: Evaluation criteria

        Returns:
            Tuple of (llm_a_initial, llm_b_review, llm_a_final or None)
        """
        print(f"\n{'='*80}")
        print(f"ðŸŽ­ Starting 3-Step Multiturn Debate Mode")
        print(f"{'='*80}\n")

        # Message history for LLM A's multiturn conversation
        llm_a_messages = []

        # Step 1: LLM A's initial evaluation
        print(f"ðŸ“ STEP 1/3: LLM A - Initial Evaluation (Multiturn Start)")

        department_info = f"{application.division or 'N/A'} > {application.department.name if application.department else 'N/A'}"

        system_message = f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ëŒ€ê¸°ì—…ì˜ AI ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì¡°ì§: {department_info}

ì—­í• : ì§€ì›ì„œ ë‚´ìš©ì„ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
2. {department_info} ì¡°ì§ì˜ ì—…ë¬´ íŠ¹ì„±ì„ ê³ ë ¤í•œ í•´ì„
3. ì‚¬ì‹¤ ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
4. ê³¼ìž¥í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ

**ì¤‘ìš”**: ê³§ ë™ë£Œ í‰ê°€ìž(LLM B)ê°€ ë‹¹ì‹ ì˜ í‰ê°€ë¥¼ ê²€í† í•  ê²ƒìž…ë‹ˆë‹¤.
ê·¸ í›„ LLM Bì˜ ì˜ê²¬ì„ ë“£ê³  ìµœì¢… í‰ê°€ë¥¼ ì¡°ì •í•  ê¸°íšŒê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤."""

        prompt_a_initial = self.build_evaluation_prompt(application, criteria_list)

        llm_a_messages.append(SystemMessage(content=system_message))
        llm_a_messages.append(HumanMessage(content=prompt_a_initial))

        # Print and invoke
        self._print_prompt("LLM A", system_message + "\n\n" + prompt_a_initial, "[Step 1/3: Initial Evaluation - Multiturn]")

        self.rate_limiter.wait_if_needed()
        response_a_initial = self.llm_a.invoke(llm_a_messages)
        content_a_initial = response_a_initial.content

        self._print_response("LLM A", content_a_initial, "[Step 1/3: Initial Evaluation]")

        # Parse Step 1 result
        json_text_a_initial = self._extract_json_from_text(content_a_initial)
        try:
            result_a_initial = json.loads(json_text_a_initial)
            print(f"âœ… LLM A Step 1 JSON parsed successfully")
        except json.JSONDecodeError as e:
            print(f"âŒ LLM A Step 1 JSON parsing error: {e}")
            raise

        # Add LLM A's response to message history
        llm_a_messages.append(AIMessage(content=content_a_initial))

        # Step 2: LLM B reviews and refines
        result_b_review = None
        result_a_final = None

        if self.llm_b:
            try:
                print(f"\nðŸ“ STEP 2/3: LLM B - Review & Refinement (Independent)")
                debate_prompt = self.build_debate_prompt(application, criteria_list, result_a_initial)
                result_b_review = self.evaluate_with_single_llm(
                    self.llm_b,
                    debate_prompt,
                    "LLM B",
                    step="[Step 2/3: Review]",
                    verbose=True
                )

                # Step 3: LLM A receives LLM B's feedback in same conversation
                print(f"\nðŸ“ STEP 3/3: LLM A - Final Decision (Multiturn Continue)")

                llm_b_summary = json.dumps(result_b_review, ensure_ascii=False, indent=2)

                feedback_prompt = f"""ì´ì œ ë™ë£Œ í‰ê°€ìž(LLM B)ê°€ ë‹¹ì‹ ì˜ í‰ê°€ë¥¼ ê²€í† í–ˆìŠµë‹ˆë‹¤.

## LLM Bì˜ ê²€í†  ì˜ê²¬:

```json
{llm_b_summary}```

## ìµœì¢… í‰ê°€ ìš”ì²­

LLM Bì˜ ê²€í†  ì˜ê²¬ì„ ê³ ë ¤í•˜ì—¬ ìµœì¢… í‰ê°€ë¥¼ ë‚´ë ¤ì£¼ì„¸ìš”.

### ê²€í†  ì‚¬í•­

1. **LLM Bì˜ ì§€ì ì´ íƒ€ë‹¹í•œê°€?**
   - ì§€ì›ì„œ ë‚´ìš©ì„ ë” ì •í™•ížˆ ë°˜ì˜í–ˆëŠ”ê°€?
   - ë†“ì¹œ ì¤‘ìš”í•œ ë‚´ìš©ì„ ë°œê²¬í–ˆëŠ”ê°€?
   - ì ìˆ˜ ì¡°ì •ì´ í•©ë¦¬ì ì¸ê°€?

2. **ë‹¹ì‹ ì˜ ì´ˆê¸° í‰ê°€ë¥¼ ìœ ì§€í•  ë¶€ë¶„ì€?**
   - LLM Bê°€ ê³¼ìž¥í•˜ê±°ë‚˜ ìž˜ëª» í•´ì„í•œ ë¶€ë¶„ì€?
   - ì´ˆê¸° í‰ê°€ê°€ ë” ê°ê´€ì ì´ì—ˆë˜ ë¶€ë¶„ì€?

3. **ìµœì¢… íŒë‹¨**
   - ê° í‰ê°€ ê¸°ì¤€ë³„ë¡œ ìµœì¢… ì ìˆ˜ì™€ ê·¼ê±° ê²°ì •
   - ë‘ í‰ê°€ë¥¼ ì¢…í•©í•œ ê· í˜•ìž¡ížŒ ê²°ê³¼ ë„ì¶œ

### ì‘ë‹µ í˜•ì‹ (JSON)

**CRITICAL**: ë°˜ë“œì‹œ ì•„ëž˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

```json
{{
  "ai_category": "ì˜ˆì¸¡",
  "business_impact": "ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼ (ìµœì¢… íŒë‹¨)",
  "technical_feasibility": "AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„± (ìµœì¢… íŒë‹¨)",
  "five_line_summary": [
    "1. ê³¼ì œ ëª©ì ",
    "2. í˜„ìž¬ ë¬¸ì œ",
    "3. í•´ê²° ë°©ì•ˆ",
    "4. ê¸°ëŒ€ íš¨ê³¼",
    "5. êµ¬í˜„ ê³„íš"
  ],
  "evaluation_scores": {{
{self._build_json_format_example(criteria_list)}
  }},
  "final_decision": "ì´ˆê¸° í‰ê°€ì™€ LLM Bì˜ ê²€í†  ì˜ê²¬ì„ ì¢…í•©í•œ ìµœì¢… íŒë‹¨ ê·¼ê±°ë¥¼ 2-3ë¬¸ìž¥ìœ¼ë¡œ ì„¤ëª…"
}}
```

**ì¤‘ìš”**:
- LLM Bì˜ ì˜ê²¬ì— ë™ì˜í•˜ë©´ ì ìˆ˜ë¥¼ ì¡°ì •í•˜ê³  ì´ìœ  ì„¤ëª…
- LLM Bì˜ ì˜ê²¬ì— ë™ì˜í•˜ì§€ ì•Šìœ¼ë©´ ì´ˆê¸° í‰ê°€ë¥¼ ìœ ì§€í•˜ê³  ì´ìœ  ì„¤ëª…
- ë¶€ë¶„ì ìœ¼ë¡œ ë™ì˜í•˜ë©´ ì ˆì¶©ì•ˆ ì œì‹œ"""

                llm_a_messages.append(HumanMessage(content=feedback_prompt))

                # Print and invoke
                self._print_prompt("LLM A", feedback_prompt, "[Step 3/3: Final Decision - Multiturn]")

                self.rate_limiter.wait_if_needed()
                response_a_final = self.llm_a.invoke(llm_a_messages)
                content_a_final = response_a_final.content

                self._print_response("LLM A", content_a_final, "[Step 3/3: Final Decision]")

                # Parse Step 3 result
                json_text_a_final = self._extract_json_from_text(content_a_final)
                try:
                    result_a_final = json.loads(json_text_a_final)
                    print(f"âœ… LLM A Step 3 JSON parsed successfully")
                except json.JSONDecodeError as e:
                    print(f"âŒ LLM A Step 3 JSON parsing error: {e}")
                    raise

                print(f"\n{'='*80}")
                print(f"âœ… 3-Step Multiturn Debate Completed")
                print(f"  - LLM A maintained conversation context across Step 1 and Step 3")
                print(f"  - Total messages in LLM A conversation: {len(llm_a_messages) + 1}")
                print(f"{'='*80}\n")

            except Exception as e:
                print(f"âš ï¸  Debate process failed at step 2 or 3: {e}")
                import traceback
                traceback.print_exc()
                print(f"â„¹ï¸  Using LLM A initial result only")

        return result_a_initial, result_b_review, result_a_final
    
    def _merge_debate_results(
        self,
        result_a_initial: Dict[str, Any],
        result_b_review: Optional[Dict[str, Any]],
        result_a_final: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Merge 3-step debate results: Use LLM A's final decision as primary

        Args:
            result_a_initial: LLM A's initial evaluation
            result_b_review: LLM B's review (optional)
            result_a_final: LLM A's final decision after considering B's review (optional)

        Returns:
            Merged result dictionary with all three perspectives
        """
        # If we have final decision from LLM A, use it as primary
        if result_a_final:
            merged = {
                "ai_category": result_a_final.get("ai_category", result_a_initial.get("ai_category", "ë¶„ë¥˜")),
                "business_impact": result_a_final.get("business_impact", result_a_initial.get("business_impact", "")),
                "technical_feasibility": result_a_final.get("technical_feasibility", result_a_initial.get("technical_feasibility", "")),
                "five_line_summary": result_a_final.get("five_line_summary", result_a_initial.get("five_line_summary", [])),
                "debate_summary": result_a_final.get("final_decision", result_b_review.get("debate_summary", "") if result_b_review else ""),
                "evaluation_scores": {}
            }

            # Merge evaluation scores with all three perspectives
            scores_a_initial = result_a_initial.get("evaluation_scores", {})
            scores_b_review = result_b_review.get("evaluation_scores", {}) if result_b_review else {}
            scores_a_final = result_a_final.get("evaluation_scores", {})

            # Get all criteria keys
            all_criteria = set(scores_a_initial.keys()) | set(scores_b_review.keys()) | set(scores_a_final.keys())

            for criterion in all_criteria:
                score_a_init_obj = scores_a_initial.get(criterion, {})
                score_b_rev_obj = scores_b_review.get(criterion, {})
                score_a_final_obj = scores_a_final.get(criterion, {})

                score_a_init = score_a_init_obj.get("score", 0) if isinstance(score_a_init_obj, dict) else 0
                score_b_rev = score_b_rev_obj.get("score", 0) if isinstance(score_b_rev_obj, dict) else 0
                score_a_final = score_a_final_obj.get("score", 0) if isinstance(score_a_final_obj, dict) else 0

                rationale_a_init = score_a_init_obj.get("rationale", "") if isinstance(score_a_init_obj, dict) else ""
                rationale_b_rev = score_b_rev_obj.get("rationale", "") if isinstance(score_b_rev_obj, dict) else ""
                rationale_a_final = score_a_final_obj.get("rationale", "") if isinstance(score_a_final_obj, dict) else ""

                # Use LLM A's final score as primary
                final_score = score_a_final if score_a_final > 0 else (score_a_init if score_a_init > 0 else 3)

                # Build comprehensive rationale showing all three steps
                rationale_parts = []
                if score_a_init > 0:
                    rationale_parts.append(f"[Step 1 - LLM A ì´ˆê¸°: {score_a_init}ì ]\n{rationale_a_init}")
                if score_b_rev > 0:
                    rationale_parts.append(f"[Step 2 - LLM B ê²€í† : {score_b_rev}ì ]\n{rationale_b_rev}")
                if score_a_final > 0:
                    rationale_parts.append(f"[Step 3 - LLM A ìµœì¢…: {score_a_final}ì ]\n{rationale_a_final}")

                combined_rationale = "\n\n".join(rationale_parts) if rationale_parts else "í‰ê°€ ì ìˆ˜ë¥¼ ì‚°ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                merged["evaluation_scores"][criterion] = {
                    "score": final_score,
                    "rationale": combined_rationale,
                    "score_a_initial": score_a_init,
                    "score_b_review": score_b_rev,
                    "score_a_final": score_a_final
                }

            print(f"âœ… Merged 3-step debate results: LLM A's final decision with full context")
            return merged

        # Fallback to 2-step if no final decision
        elif result_b_review:
            return self._merge_2step_results(result_a_initial, result_b_review)

        # Fallback to initial result if debate failed
        else:
            return result_a_initial

    def _merge_2step_results(self, result_a: Dict[str, Any], result_b: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback: Merge 2-step results (for backward compatibility)"""
        merged = {
            "ai_category": result_b.get("ai_category", result_a.get("ai_category", "ë¶„ë¥˜")),
            "business_impact": result_b.get("business_impact", result_a.get("business_impact", "")),
            "technical_feasibility": result_b.get("technical_feasibility", result_a.get("technical_feasibility", "")),
            "five_line_summary": result_b.get("five_line_summary", result_a.get("five_line_summary", [])),
            "debate_summary": result_b.get("debate_summary", ""),
            "evaluation_scores": {}
        }

        scores_a = result_a.get("evaluation_scores", {})
        scores_b = result_b.get("evaluation_scores", {})
        all_criteria = set(scores_a.keys()) | set(scores_b.keys())

        for criterion in all_criteria:
            score_a_obj = scores_a.get(criterion, {})
            score_b_obj = scores_b.get(criterion, {})

            score_a = score_a_obj.get("score", 0) if isinstance(score_a_obj, dict) else 0
            score_b = score_b_obj.get("score", 0) if isinstance(score_b_obj, dict) else 0

            rationale_a = score_a_obj.get("rationale", "") if isinstance(score_a_obj, dict) else ""
            rationale_b = score_b_obj.get("rationale", "") if isinstance(score_b_obj, dict) else ""

            final_score = score_b if score_b > 0 else (score_a if score_a > 0 else 3)

            if score_a > 0 and score_b > 0 and score_a != score_b:
                combined_rationale = f"[LLM A ì´ˆê¸°: {score_a}ì ]\n{rationale_a}\n\n[LLM B ê²€í† : {score_b}ì ]\n{rationale_b}"
            elif score_b > 0:
                combined_rationale = f"[í•©ì˜: {score_b}ì ]\n{rationale_b}"
            else:
                combined_rationale = rationale_a

            merged["evaluation_scores"][criterion] = {
                "score": final_score,
                "rationale": combined_rationale,
                "score_a": score_a,
                "score_b": score_b
            }

        return merged

    def calculate_overall_grade(self, evaluation_detail: Dict[str, Any]) -> str:
        """
        Calculate overall grade from evaluation details
        
        Args:
            evaluation_detail: Dictionary of evaluation scores
            
        Returns:
            Overall grade (S/A/B/C/D)
        """
        total_score = 0
        count = 0
        
        for item in evaluation_detail.values():
            if isinstance(item, dict) and "score" in item:
                total_score += item["score"]
                count += 1
        
        if count == 0:
            return "C"
        
        avg_score = total_score / count
        
        if avg_score >= 4.5:
            return "S"
        elif avg_score >= 3.5:
            return "A"
        elif avg_score >= 2.5:
            return "B"
        elif avg_score >= 1.5:
            return "C"
        else:
            return "D"
    
    def evaluate_application(
        self, 
        db: Session, 
        application: Application,
        criteria_list: Optional[List[EvaluationCriteria]] = None
    ) -> bool:
        """
        Evaluate single application
        
        Args:
            db: Database session
            application: Application to evaluate
            criteria_list: Evaluation criteria (optional, will fetch if None)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get evaluation criteria if not provided (backward compatibility)
            if criteria_list is None:
                criteria_list = db.query(EvaluationCriteria).filter(
                    EvaluationCriteria.is_active == True
                ).order_by(EvaluationCriteria.display_order).all()
            
            # Evaluate with LLM(s)
            print(f"ðŸ¤– Evaluating application {application.id} ({application.subject})...")

            if self.llm_b:
                # 3-Step Multiturn Debate mode: LLM A â†’ LLM B â†’ LLM A (with conversation context)
                print(f"ðŸ’¬ Using 3-step multiturn debate mode: LLM A (multiturn) â†’ LLM B â†’ LLM A (continue conversation)")
                result_a_initial, result_b_review, result_a_final = self.evaluate_with_multiturn_debate(
                    application,
                    criteria_list or []
                )

                # Merge results
                result = self._merge_debate_results(result_a_initial, result_b_review, result_a_final)
            else:
                # Single LLM mode
                print(f"ðŸ¤– Using single LLM mode")
                prompt = self.build_evaluation_prompt(application, criteria_list or [])
                result_a = self.evaluate_with_single_llm(
                    self.llm_a,
                    prompt,
                    "LLM A",
                    step="[Single LLM Mode]"
                )
                result = result_a

            # Extract results
            ai_category = result.get("ai_category", "ë¶„ë¥˜")
            business_impact = result.get("business_impact", "")
            technical_feasibility = result.get("technical_feasibility", "")
            five_line_summary = result.get("five_line_summary", [])
            evaluation_scores = result.get("evaluation_scores", {})

            # Build AI categories for compatibility
            ai_categories = [{
                "category": ai_category,
                "description": "ì§€ì›ì„œ ê¸°ë°˜ AI ìš”ì•½"
            }]

            # Build evaluation detail with scores
            evaluation_detail = {
                "ai_category": ai_category,
                "business_impact": business_impact,
                "technical_feasibility": technical_feasibility,
                "five_line_summary": five_line_summary,
                "evaluation_scores": evaluation_scores
            }

            # Calculate overall grade from evaluation scores
            if evaluation_scores:
                scores = []
                for criterion in ["innovation", "feasibility", "impact", "clarity"]:
                    if criterion in evaluation_scores and "score" in evaluation_scores[criterion]:
                        scores.append(evaluation_scores[criterion]["score"])

                if scores:
                    avg_score = sum(scores) / len(scores)
                    # Convert average to grade (S/A/B/C/D)
                    if avg_score >= 4.5:
                        overall_grade = "S"
                    elif avg_score >= 3.5:
                        overall_grade = "A"
                    elif avg_score >= 2.5:
                        overall_grade = "B"
                    elif avg_score >= 1.5:
                        overall_grade = "C"
                    else:
                        overall_grade = "D"
                else:
                    overall_grade = "B"  # Default
            else:
                # Fallback to old simple logic if scores not provided
                if "ì–´ë µ" in technical_feasibility or "ë¶ˆê°€ëŠ¥" in technical_feasibility:
                    overall_grade = "C"
                elif "ê°€ëŠ¥" in technical_feasibility and "ì¶©ë¶„" in technical_feasibility:
                    overall_grade = "A"
                else:
                    overall_grade = "B"
            
            # Build summary
            summary_parts = []
            summary_parts.append(f"**AI ê¸°ìˆ  ë¶„ë¥˜**: {ai_category}\n\n")
            summary_parts.append(f"**ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼**\n{business_impact}\n\n")
            summary_parts.append(f"**AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±**\n{technical_feasibility}\n\n")
            summary_parts.append(f"**ì „ì²´ ì§€ì›ì„œ 5ì¤„ ìš”ì•½**\n" + "\n".join(five_line_summary))
            
            summary = "".join(summary_parts)
            
            # Update application
            application.ai_categories = ai_categories
            application.ai_category_primary = ai_category
            application.ai_evaluation_detail = evaluation_detail
            application.ai_grade = overall_grade
            application.ai_summary = summary
            application.ai_evaluated_at = datetime.utcnow()
            application.status = "ai_evaluated"
            
            # Save evaluation history
            history = EvaluationHistory(
                application_id=application.id,
                evaluator_id=None,
                evaluator_type="AI",
                grade=overall_grade,
                summary=summary,
                evaluation_detail=evaluation_detail,
                ai_categories=ai_categories
            )
            db.add(history)
            
            db.commit()
            print(f"âœ… Application {application.id} evaluated: {overall_grade} ({ai_category})")
            return True
            
        except Exception as e:
            print(f"âŒ Error evaluating application {application.id}: {e}")
            import traceback
            traceback.print_exc()
            db.rollback()
            return False
    
    def _score_to_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 4.5:
            return "S"
        elif score >= 3.5:
            return "A"
        elif score >= 2.5:
            return "B"
        elif score >= 1.5:
            return "C"
        else:
            return "D"


# Singleton instance
llm_evaluator = LLMEvaluator()

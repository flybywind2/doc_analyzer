"""
LLM Evaluator Service
"""
import uuid
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from app.config import settings
from app.models.application import Application
from app.models.evaluation import EvaluationCriteria, EvaluationHistory


class LLMEvaluator:
    """LLM-based application evaluator"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            base_url=settings.llm_api_base_url,
            api_key=settings.llm_api_key,
            model=settings.llm_model_name,
            temperature=0.1,  # ì¼ê´€ì„± í™•ë³´
            default_headers={
                "x-dep-ticket": settings.llm_credential_key,
                "Send-System-Name": settings.llm_system_name,
                "User-ID": settings.llm_user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )
    
    def build_evaluation_prompt(
        self, 
        application: Application, 
        criteria_list: List[EvaluationCriteria]
    ) -> str:
        """
        Build evaluation prompt for LLM
        
        Args:
            application: Application to evaluate
            criteria_list: List of evaluation criteria
            
        Returns:
            Formatted prompt string
        """
        # ê³¼ì œ ì •ë³´ êµ¬ì„±
        department_info = f"{application.division or 'N/A'} > {application.department.name if application.department else 'N/A'}"
        
        system_prompt = f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ëŒ€ê¸°ì—…ì˜ AI ê³¼ì œ ì‹¬ì‚¬ ë‹´ë‹¹ìžìž…ë‹ˆë‹¤.
ì¡°ì§: {department_info}

ë‹¹ì‹ ì˜ ì—­í• :
- í•´ë‹¹ ì¡°ì§ì˜ ê´€ì ì—ì„œ AI ê³¼ì œì˜ ì‚¬ì—…ì  ê°€ì¹˜(Biz Impact)ì™€ ì‹¤í˜„ ê°€ëŠ¥ì„±(Feasibility)ì„ í‰ê°€
- ì‹¬ì‚¬ìœ„ì›ë“¤ì´ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìžˆë„ë¡ ê³¼ì œì˜ ê°•ì ì„ ë¶€ê°
- ì¡°ì§ì˜ ì—…ë¬´ íŠ¹ì„±ê³¼ ì „ëžµì  ë°©í–¥ì„±ì„ ê³ ë ¤í•œ í‰ê°€

í‰ê°€ ì›ì¹™:
1. ì‚¬ì—…ë¶€ì™€ ë¶€ì„œì˜ íŠ¹ì„±ì„ ë°˜ì˜í•œ ë§žì¶¤í˜• í‰ê°€
2. ì‹¤ì§ˆì ì¸ ì—…ë¬´ ê°œì„  íš¨ê³¼ì— ì´ˆì 
3. ê¸°ìˆ ì  ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ í˜„ì‹¤ì ìœ¼ë¡œ í‰ê°€
4. ì‹¬ì‚¬ìœ„ì›ì˜ í‰ê°€ë¥¼ ì§€ì›í•˜ëŠ” ê´€ì ì—ì„œ ìž‘ì„±
"""

        app_info = f"""
# AI ê³¼ì œ ì§€ì›ì„œ í‰ê°€

## ê³¼ì œ ê¸°ë³¸ ì •ë³´
- ê³¼ì œëª…: {application.subject or 'N/A'}
- ì¡°ì§: {department_info}
- ì°¸ì—¬ ì¸ì›: {application.participant_count or 'N/A'}ëª…
- ëŒ€í‘œìž: {application.representative_name or 'N/A'}

## ì‹ ì²­ ë‚´ìš©
### í˜„ìž¬ ì—…ë¬´
{application.current_work or 'N/A'}

### Pain Point (í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œ)
{application.pain_point or 'N/A'}

### ê°œì„  ì•„ì´ë””ì–´
{application.improvement_idea or 'N/A'}

### ê¸°ëŒ€ íš¨ê³¼
{application.expected_effect or 'N/A'}

### ë°”ë¼ëŠ” ì 
{application.hope or 'N/A'}

## ì‚¬ì „ ì„¤ë¬¸
{json.dumps(application.pre_survey, ensure_ascii=False, indent=2) if application.pre_survey else 'N/A'}

## ì°¸ì—¬ìž ê¸°ìˆ  ì—­ëŸ‰
{json.dumps(application.tech_capabilities, ensure_ascii=False, indent=2) if application.tech_capabilities else 'N/A'}

---

## í‰ê°€ ìš”ì²­ì‚¬í•­

ë‹¤ìŒ 4ê°€ì§€ ê´€ì ì—ì„œ í‰ê°€í•´ì£¼ì„¸ìš”:

### 1. AI ê¸°ìˆ  ë¶„ë¥˜
ê³¼ì œì—ì„œ í™œìš©í•˜ë ¤ëŠ” AI ê¸°ìˆ ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ê³ , ì„ íƒ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”:
- **ML (Machine Learning)**: ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡, ë¶„ë¥˜, íšŒê·€ ë¶„ì„ ë“±
- **ì±—ë´‡ (Chatbot)**: ëŒ€í™”í˜• ì¸í„°íŽ˜ì´ìŠ¤, ìžë™ ì‘ë‹µ, Q&A ì‹œìŠ¤í…œ ë“±
- **Agent**: ìžìœ¨ì  ì˜ì‚¬ê²°ì •, ë³µìž¡í•œ ìž‘ì—… ìžë™í™”, ë©€í‹°ìŠ¤í… í”„ë¡œì„¸ìŠ¤ ë“±

### 2. Biz Impact (ì‚¬ì—… ì˜í–¥ë„)
{department_info} ì¡°ì§ ê´€ì ì—ì„œ:
- ì—…ë¬´ íš¨ìœ¨ì„± ê°œì„  ì •ë„
- ë¹„ìš© ì ˆê° ë˜ëŠ” ë§¤ì¶œ ì¦ëŒ€ íš¨ê³¼
- ì¡°ì§ ì „ëžµê³¼ì˜ ì—°ê³„ì„±
- ì •ëŸ‰ì  íš¨ê³¼ (ê°€ëŠ¥í•œ ê²½ìš°)

### 3. Feasibility (ì‹¤í˜„ ê°€ëŠ¥ì„±)
- ê¸°ìˆ ì  ë‚œì´ë„ì™€ í˜„ìž¬ ê¸°ìˆ  ìˆ˜ì¤€
- í•„ìš”í•œ ë°ì´í„°ì˜ í™•ë³´ ê°€ëŠ¥ì„±
- ì°¸ì—¬ ì¸ì›ì˜ ì—­ëŸ‰ê³¼ ê³¼ì œ ìš”êµ¬ì‚¬í•­ ë¶€í•©ë„
- ì˜ˆìƒ ê°œë°œ ê¸°ê°„ê³¼ ë¦¬ì†ŒìŠ¤
- ìž ìž¬ì  ìœ„í—˜ ìš”ì†Œì™€ ëŒ€ì‘ ë°©ì•ˆ

### 4. ì „ë°˜ì ì¸ AI ìš”ì•½
ì‹¬ì‚¬ìœ„ì›ì´ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìžˆë„ë¡:
- ê³¼ì œì˜ í•µì‹¬ ê°€ì¹˜ (3-5ì¤„)
- ì¶”ì²œ ì´ìœ  ë˜ëŠ” ê³ ë ¤ì‚¬í•­
- ì‹¬ì‚¬ ì‹œ ì£¼ëª©í•  í¬ì¸íŠ¸
"""
        
        prompt = f"""{system_prompt}

{app_info}

---

## ì‘ë‹µ í˜•ì‹ (JSON)
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•ížˆ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "ai_technology_category": {{
    "category": "ML" ë˜ëŠ” "ì±—ë´‡" ë˜ëŠ” "Agent",
    "reason": "ì´ ê¸°ìˆ ë¡œ ë¶„ë¥˜í•œ ì´ìœ ë¥¼ 2-3ë¬¸ìž¥ìœ¼ë¡œ ì„¤ëª…",
    "confidence": 0.9  // 0.0 ~ 1.0 ì‚¬ì´ í™•ì‹ ë„
  }},
  "biz_impact": {{
    "score": 4.5,  // 1.0 ~ 5.0
    "summary": "ì‚¬ì—… ì˜í–¥ë„ ìš”ì•½ (3-5ì¤„)",
    "key_benefits": [
      "í•µì‹¬ ì´ì  1",
      "í•µì‹¬ ì´ì  2",
      "í•µì‹¬ ì´ì  3"
    ],
    "strategic_alignment": "ì¡°ì§ ì „ëžµê³¼ì˜ ì—°ê³„ì„± ì„¤ëª… (2-3ì¤„)"
  }},
  "feasibility": {{
    "score": 3.8,  // 1.0 ~ 5.0
    "summary": "ì‹¤í˜„ ê°€ëŠ¥ì„± ìš”ì•½ (3-5ì¤„)",
    "technical_difficulty": "ìƒ/ì¤‘/í•˜ ì¤‘ í•˜ë‚˜ì™€ ì´ìœ ",
    "data_availability": "ë°ì´í„° í™•ë³´ ê°€ëŠ¥ì„± í‰ê°€",
    "team_capability": "íŒ€ ì—­ëŸ‰ í‰ê°€",
    "risks": [
      "ìœ„í—˜ ìš”ì†Œ 1",
      "ìœ„í—˜ ìš”ì†Œ 2"
    ],
    "timeline_estimate": "ì˜ˆìƒ ê°œë°œ ê¸°ê°„ (ì˜ˆ: 3-6ê°œì›”)"
  }},
  "overall_summary": {{
    "recommendation": "ê°•ë ¥ ì¶”ì²œ / ì¶”ì²œ / ì¡°ê±´ë¶€ ì¶”ì²œ / ë³´ë¥˜ ì¤‘ í•˜ë‚˜",
    "core_value": "ê³¼ì œì˜ í•µì‹¬ ê°€ì¹˜ ì„¤ëª… (3-5ì¤„)",
    "review_points": [
      "ì‹¬ì‚¬ ì‹œ ì£¼ëª©í•  í¬ì¸íŠ¸ 1",
      "ì‹¬ì‚¬ ì‹œ ì£¼ëª©í•  í¬ì¸íŠ¸ 2",
      "ì‹¬ì‚¬ ì‹œ ì£¼ëª©í•  í¬ì¸íŠ¸ 3"
    ],
    "final_comment": "ìµœì¢… í•œì¤„ í‰ê°€"
  }}
}}

**ì¤‘ìš”ì‚¬í•­:**
1. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”
2. ëª¨ë“  í•„ë“œë¥¼ ë¹ ì§ì—†ì´ ì±„ì›Œì£¼ì„¸ìš”
3. scoreëŠ” ë°˜ë“œì‹œ ìˆ«ìž(float)ë¡œ ìž‘ì„±í•˜ì„¸ìš”
4. {department_info} ì¡°ì§ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì—¬ í‰ê°€í•˜ì„¸ìš”
5. ì‹¬ì‚¬ìœ„ì›ì´ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìžˆë„ë¡ ê°•ì ì„ ë¶€ê°í•˜ì„¸ìš”
"""
        return prompt
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type(Exception)
    )
    def evaluate_with_llm(self, prompt: str) -> Dict[str, Any]:
        """
        Evaluate application using LLM with retry logic
        
        Args:
            prompt: Evaluation prompt
            
        Returns:
            Evaluation result dictionary
            
        Raises:
            Exception: If evaluation fails after retries
        """
        response = self.llm.invoke(prompt)
        content = response.content
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            # Markdown code block ì œê±°
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            result = json.loads(content.strip())
            return result
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parsing error: {e}")
            print(f"Response content: {content}")
            raise
    
    def calculate_overall_grade(self, evaluation_detail: Dict[str, Any]) -> str:
        """
        Calculate overall grade from evaluation details
        
        Args:
            evaluation_detail: Dictionary of evaluation scores
            
        Returns:
            Overall grade (S/A/B/C/D)
        """
        total_score = 0
        count = 0
        
        for item in evaluation_detail.values():
            if isinstance(item, dict) and "score" in item:
                total_score += item["score"]
                count += 1
        
        if count == 0:
            return "C"
        
        avg_score = total_score / count
        
        if avg_score >= 4.5:
            return "S"
        elif avg_score >= 3.5:
            return "A"
        elif avg_score >= 2.5:
            return "B"
        elif avg_score >= 1.5:
            return "C"
        else:
            return "D"
    
    def evaluate_application(
        self, 
        db: Session, 
        application: Application,
        criteria_list: Optional[List[EvaluationCriteria]] = None
    ) -> bool:
        """
        Evaluate single application
        
        Args:
            db: Database session
            application: Application to evaluate
            criteria_list: Evaluation criteria (optional, will fetch if None)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get evaluation criteria if not provided (backward compatibility)
            if criteria_list is None:
                criteria_list = db.query(EvaluationCriteria).filter(
                    EvaluationCriteria.is_active == True
                ).order_by(EvaluationCriteria.display_order).all()
            
            # Build prompt
            prompt = self.build_evaluation_prompt(application, criteria_list or [])
            
            # Evaluate with LLM
            print(f"ðŸ¤– Evaluating application {application.id} ({application.subject})...")
            result = self.evaluate_with_llm(prompt)
            
            # Extract new format results
            ai_tech = result.get("ai_technology_category", {})
            biz_impact = result.get("biz_impact", {})
            feasibility = result.get("feasibility", {})
            overall = result.get("overall_summary", {})
            
            # Build AI categories for compatibility
            ai_categories = [{
                "category": ai_tech.get("category", "Unknown"),
                "confidence": ai_tech.get("confidence", 0.0),
                "reason": ai_tech.get("reason", "")
            }]
            
            # Build evaluation detail for new format
            evaluation_detail = {
                "ai_technology": ai_tech,
                "biz_impact": biz_impact,
                "feasibility": feasibility,
                "overall_summary": overall,
                "scores": {
                    "Biz Impact": {
                        "score": biz_impact.get("score", 3.0),
                        "grade": self._score_to_grade(biz_impact.get("score", 3.0))
                    },
                    "Feasibility": {
                        "score": feasibility.get("score", 3.0),
                        "grade": self._score_to_grade(feasibility.get("score", 3.0))
                    }
                }
            }
            
            # Calculate overall grade from biz_impact and feasibility scores
            avg_score = (biz_impact.get("score", 3.0) + feasibility.get("score", 3.0)) / 2
            overall_grade = self._score_to_grade(avg_score)
            
            # Build summary
            summary_parts = []
            summary_parts.append(f"**AI ê¸°ìˆ  ë¶„ë¥˜**: {ai_tech.get('category', 'Unknown')}")
            summary_parts.append(f"\n**Biz Impact**: {biz_impact.get('summary', 'N/A')}")
            summary_parts.append(f"\n**Feasibility**: {feasibility.get('summary', 'N/A')}")
            summary_parts.append(f"\n**ì¶”ì²œ**: {overall.get('recommendation', 'N/A')}")
            summary_parts.append(f"\n\n{overall.get('core_value', '')}")
            
            summary = "".join(summary_parts)
            
            # Update application
            application.ai_categories = ai_categories
            application.ai_category_primary = ai_tech.get("category", "Unknown")
            application.ai_evaluation_detail = evaluation_detail
            application.ai_grade = overall_grade
            application.ai_summary = summary
            application.ai_evaluated_at = datetime.utcnow()
            application.status = "ai_evaluated"
            
            # Save evaluation history
            history = EvaluationHistory(
                application_id=application.id,
                evaluator_id=None,
                evaluator_type="AI",
                grade=overall_grade,
                summary=summary,
                evaluation_detail=evaluation_detail,
                ai_categories=ai_categories
            )
            db.add(history)
            
            db.commit()
            print(f"âœ… Application {application.id} evaluated: {overall_grade} ({ai_tech.get('category', 'Unknown')})")
            return True
            
        except Exception as e:
            print(f"âŒ Error evaluating application {application.id}: {e}")
            import traceback
            traceback.print_exc()
            db.rollback()
            return False
    
    def _score_to_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 4.5:
            return "S"
        elif score >= 3.5:
            return "A"
        elif score >= 2.5:
            return "B"
        elif score >= 1.5:
            return "C"
        else:
            return "D"


# Singleton instance
llm_evaluator = LLMEvaluator()

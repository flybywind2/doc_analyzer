"""
LLM Evaluator Service
"""
import uuid
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from app.config import settings
from app.models.application import Application
from app.models.evaluation import EvaluationCriteria, EvaluationHistory


class LLMEvaluator:
    """LLM-based application evaluator"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            base_url=settings.llm_api_base_url,
            api_key=settings.llm_api_key,
            model=settings.llm_model_name,
            temperature=0.1,  # ì¼ê´€ì„± í™•ë³´
            default_headers={
                "x-dep-ticket": settings.llm_credential_key,
                "Send-System-Name": settings.llm_system_name,
                "User-ID": settings.llm_user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )
    
    def build_evaluation_prompt(
        self, 
        application: Application, 
        criteria_list: List[EvaluationCriteria]
    ) -> str:
        """
        Build evaluation prompt for LLM
        
        Args:
            application: Application to evaluate
            criteria_list: List of evaluation criteria
            
        Returns:
            Formatted prompt string
        """
        # ê³¼ì œ ì •ë³´ êµ¬ì„±
        department_info = f"{application.division or 'N/A'} > {application.department.name if application.department else 'N/A'}"
        
        system_prompt = f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ëŒ€ê¸°ì—…ì˜ AI ê³¼ì œ ì‹¬ì‚¬ ë‹´ë‹¹ìžìž…ë‹ˆë‹¤.
ì¡°ì§: {department_info}

ë‹¹ì‹ ì˜ ì—­í• :
- í•´ë‹¹ ì¡°ì§ì˜ ê´€ì ì—ì„œ AI ê³¼ì œì˜ ì‚¬ì—…ì  ê°€ì¹˜(Biz Impact)ì™€ ì‹¤í˜„ ê°€ëŠ¥ì„±(Feasibility)ì„ í‰ê°€
- ì‹¬ì‚¬ìœ„ì›ë“¤ì´ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìžˆë„ë¡ ê³¼ì œì˜ ê°•ì ì„ ë¶€ê°
- ì¡°ì§ì˜ ì—…ë¬´ íŠ¹ì„±ê³¼ ì „ëžµì  ë°©í–¥ì„±ì„ ê³ ë ¤í•œ í‰ê°€

í‰ê°€ ì›ì¹™:
1. ì‚¬ì—…ë¶€ì™€ ë¶€ì„œì˜ íŠ¹ì„±ì„ ë°˜ì˜í•œ ë§žì¶¤í˜• í‰ê°€
2. ì‹¤ì§ˆì ì¸ ì—…ë¬´ ê°œì„  íš¨ê³¼ì— ì´ˆì 
3. ê¸°ìˆ ì  ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ í˜„ì‹¤ì ìœ¼ë¡œ í‰ê°€
4. ì‹¬ì‚¬ìœ„ì›ì˜ í‰ê°€ë¥¼ ì§€ì›í•˜ëŠ” ê´€ì ì—ì„œ ìž‘ì„±
"""

        app_info = f"""
# AI ê³¼ì œ ì§€ì›ì„œ í‰ê°€

## ê³¼ì œ ê¸°ë³¸ ì •ë³´
- ê³¼ì œëª…: {application.subject or 'N/A'}
- ì¡°ì§: {department_info}
- ì°¸ì—¬ ì¸ì›: {application.participant_count or 'N/A'}ëª…
- ëŒ€í‘œìž: {application.representative_name or 'N/A'}

## ì‹ ì²­ ë‚´ìš©
### í˜„ìž¬ ì—…ë¬´
{application.current_work or 'N/A'}

### Pain Point (í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œ)
{application.pain_point or 'N/A'}

### ê°œì„  ì•„ì´ë””ì–´
{application.improvement_idea or 'N/A'}

### ê¸°ëŒ€ íš¨ê³¼
{application.expected_effect or 'N/A'}

### ë°”ë¼ëŠ” ì 
{application.hope or 'N/A'}

## ì‚¬ì „ ì„¤ë¬¸
{json.dumps(application.pre_survey, ensure_ascii=False, indent=2) if application.pre_survey else 'N/A'}

## ì°¸ì—¬ìž ê¸°ìˆ  ì—­ëŸ‰
{json.dumps(application.tech_capabilities, ensure_ascii=False, indent=2) if application.tech_capabilities else 'N/A'}

---

## í‰ê°€ ìš”ì²­ì‚¬í•­

ë‹¤ìŒ 4ê°€ì§€ ê´€ì ì—ì„œ í‰ê°€í•´ì£¼ì„¸ìš”:

### 1. AI ê¸°ìˆ  ë¶„ë¥˜
ê³¼ì œì—ì„œ í™œìš©í•˜ë ¤ëŠ” AI ê¸°ìˆ ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ê³ , ì„ íƒ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”:
- **ML (Machine Learning)**: ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡, ë¶„ë¥˜, íšŒê·€ ë¶„ì„ ë“±
- **ì±—ë´‡ (Chatbot)**: ëŒ€í™”í˜• ì¸í„°íŽ˜ì´ìŠ¤, ìžë™ ì‘ë‹µ, Q&A ì‹œìŠ¤í…œ ë“±
- **Agent**: ìžìœ¨ì  ì˜ì‚¬ê²°ì •, ë³µìž¡í•œ ìž‘ì—… ìžë™í™”, ë©€í‹°ìŠ¤í… í”„ë¡œì„¸ìŠ¤ ë“±

### 2. ì™œ ì´ ê³¼ì œë¥¼ ì¶”ì²œí•˜ëŠ”ê°€?
{department_info} ì¡°ì§ ê´€ì ì—ì„œ ì´ ê³¼ì œê°€ ê°€ì¹˜ ìžˆëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”:
- ì–´ë–¤ ì—…ë¬´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë‚˜ìš”?
- ì¡°ì§ì— ì–´ë–¤ ê¸ì •ì  ì˜í–¥ì„ ì£¼ë‚˜ìš”?
- ì™œ ì§€ê¸ˆ ì´ ê³¼ì œê°€ í•„ìš”í•œê°€ìš”?
- ì´ ê³¼ì œì˜ í•µì‹¬ ê°€ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

### 3. ì‹¤í˜„ ê°€ëŠ¥ì„±ì€ ì–´ë–¤ê°€ìš”?
ì‹¤ì œë¡œ êµ¬í˜„í•  ìˆ˜ ìžˆì„ì§€ í˜„ì‹¤ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
- ê¸°ìˆ ì ìœ¼ë¡œ ê°€ëŠ¥í•œ ê³¼ì œì¸ê°€ìš”? (ë‚œì´ë„: ìƒ/ì¤‘/í•˜)
- í•„ìš”í•œ ë°ì´í„°ë¥¼ í™•ë³´í•  ìˆ˜ ìžˆë‚˜ìš”?
- íŒ€ì˜ ì—­ëŸ‰ì´ ì¶©ë¶„í•œê°€ìš”?
- ì–´ë–¤ ì–´ë ¤ì›€ì´ ì˜ˆìƒë˜ë‚˜ìš”?
- ì–¸ì œì¯¤ ì™„ì„±í•  ìˆ˜ ìžˆì„ê¹Œìš”?

### 4. í•œì¤„ ìš”ì•½
ì´ ê³¼ì œë¥¼ í•œ ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½í•˜ê³  ì¶”ì²œë„ë¥¼ ì œì‹œí•˜ì„¸ìš”.
"""
        
        prompt = f"""{system_prompt}

{app_info}

---

## ì‘ë‹µ í˜•ì‹ (JSON)
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•ížˆ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "ai_technology_category": {{
    "category": "ML" ë˜ëŠ” "ì±—ë´‡" ë˜ëŠ” "Agent",
    "reason": "ì´ ê¸°ìˆ ë¡œ ë¶„ë¥˜í•œ ì´ìœ ë¥¼ 2-3ë¬¸ìž¥ìœ¼ë¡œ ì„¤ëª…"
  }},
  "why_recommend": {{
    "problem_solving": "ì–´ë–¤ ì—…ë¬´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë‚˜ìš”? (2-3ë¬¸ìž¥)",
    "positive_impact": "ì¡°ì§ì— ì–´ë–¤ ê¸ì •ì  ì˜í–¥ì„ ì£¼ë‚˜ìš”? (2-3ë¬¸ìž¥)",
    "urgency": "ì™œ ì§€ê¸ˆ ì´ ê³¼ì œê°€ í•„ìš”í•œê°€ìš”? (1-2ë¬¸ìž¥)",
    "core_value": "ì´ ê³¼ì œì˜ í•µì‹¬ ê°€ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”? (2-3ë¬¸ìž¥)"
  }},
  "feasibility_assessment": {{
    "is_feasible": true,  // true or false
    "technical_difficulty": "ìƒ" ë˜ëŠ” "ì¤‘" ë˜ëŠ” "í•˜",
    "difficulty_reason": "ë‚œì´ë„ íŒë‹¨ ì´ìœ  (2-3ë¬¸ìž¥)",
    "data_availability": "í•„ìš”í•œ ë°ì´í„°ë¥¼ í™•ë³´í•  ìˆ˜ ìžˆë‚˜ìš”? (2-3ë¬¸ìž¥)",
    "team_capability": "íŒ€ì˜ ì—­ëŸ‰ì´ ì¶©ë¶„í•œê°€ìš”? (2-3ë¬¸ìž¥)",
    "expected_challenges": [
      "ì˜ˆìƒ ì–´ë ¤ì›€ 1",
      "ì˜ˆìƒ ì–´ë ¤ì›€ 2"
    ],
    "timeline_estimate": "ì–¸ì œì¯¤ ì™„ì„±í•  ìˆ˜ ìžˆì„ê¹Œìš”? (ì˜ˆ: 3-6ê°œì›”)"
  }},
  "one_line_summary": {{
    "summary": "ì´ ê³¼ì œë¥¼ í•œ ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½ (1ë¬¸ìž¥, í•µì‹¬ë§Œ)",
    "recommendation": "ê°•ë ¥ ì¶”ì²œ" ë˜ëŠ” "ì¶”ì²œ" ë˜ëŠ” "ì¡°ê±´ë¶€ ì¶”ì²œ" ë˜ëŠ” "ë³´ë¥˜",
    "recommendation_reason": "ì¶”ì²œë„ë¥¼ ì„ íƒí•œ ì´ìœ  (1-2ë¬¸ìž¥)"
  }}
}}

**ì¤‘ìš”ì‚¬í•­:**
1. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”
2. ëª¨ë“  í•„ë“œë¥¼ ë¹ ì§ì—†ì´ ì±„ì›Œì£¼ì„¸ìš”
3. ì ìˆ˜(score)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” - ì‹¬ì‚¬ìœ„ì›ì´ ìˆ«ìžì— ì˜í–¥ë°›ì§€ ì•Šë„ë¡
4. {department_info} ì¡°ì§ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì—¬ í‰ê°€í•˜ì„¸ìš”
5. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš” - ì‹¬ì‚¬ìœ„ì›ì´ ë¹ ë¥´ê²Œ ì´í•´í•  ìˆ˜ ìžˆë„ë¡
6. ê¸ì •ì ì´ê³  ê±´ì„¤ì ì¸ ê´€ì ìœ¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”
"""
        return prompt
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type(Exception)
    )
    def evaluate_with_llm(self, prompt: str) -> Dict[str, Any]:
        """
        Evaluate application using LLM with retry logic
        
        Args:
            prompt: Evaluation prompt
            
        Returns:
            Evaluation result dictionary
            
        Raises:
            Exception: If evaluation fails after retries
        """
        response = self.llm.invoke(prompt)
        content = response.content
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            # Markdown code block ì œê±°
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            result = json.loads(content.strip())
            return result
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parsing error: {e}")
            print(f"Response content: {content}")
            raise
    
    def calculate_overall_grade(self, evaluation_detail: Dict[str, Any]) -> str:
        """
        Calculate overall grade from evaluation details
        
        Args:
            evaluation_detail: Dictionary of evaluation scores
            
        Returns:
            Overall grade (S/A/B/C/D)
        """
        total_score = 0
        count = 0
        
        for item in evaluation_detail.values():
            if isinstance(item, dict) and "score" in item:
                total_score += item["score"]
                count += 1
        
        if count == 0:
            return "C"
        
        avg_score = total_score / count
        
        if avg_score >= 4.5:
            return "S"
        elif avg_score >= 3.5:
            return "A"
        elif avg_score >= 2.5:
            return "B"
        elif avg_score >= 1.5:
            return "C"
        else:
            return "D"
    
    def evaluate_application(
        self, 
        db: Session, 
        application: Application,
        criteria_list: Optional[List[EvaluationCriteria]] = None
    ) -> bool:
        """
        Evaluate single application
        
        Args:
            db: Database session
            application: Application to evaluate
            criteria_list: Evaluation criteria (optional, will fetch if None)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get evaluation criteria if not provided (backward compatibility)
            if criteria_list is None:
                criteria_list = db.query(EvaluationCriteria).filter(
                    EvaluationCriteria.is_active == True
                ).order_by(EvaluationCriteria.display_order).all()
            
            # Build prompt
            prompt = self.build_evaluation_prompt(application, criteria_list or [])
            
            # Evaluate with LLM
            print(f"ðŸ¤– Evaluating application {application.id} ({application.subject})...")
            result = self.evaluate_with_llm(prompt)
            
            # Extract new format results
            ai_tech = result.get("ai_technology_category", {})
            why_recommend = result.get("why_recommend", {})
            feasibility = result.get("feasibility_assessment", {})
            one_liner = result.get("one_line_summary", {})
            
            # Build AI categories for compatibility
            ai_categories = [{
                "category": ai_tech.get("category", "Unknown"),
                "reason": ai_tech.get("reason", "")
            }]
            
            # Build evaluation detail for new format (NO SCORES)
            evaluation_detail = {
                "ai_technology": ai_tech,
                "why_recommend": why_recommend,
                "feasibility_assessment": feasibility,
                "one_line_summary": one_liner
            }
            
            # Map recommendation to grade (without showing numeric scores)
            recommendation = one_liner.get("recommendation", "ì¶”ì²œ")
            if recommendation == "ê°•ë ¥ ì¶”ì²œ":
                overall_grade = "S"
            elif recommendation == "ì¶”ì²œ":
                overall_grade = "A"
            elif recommendation == "ì¡°ê±´ë¶€ ì¶”ì²œ":
                overall_grade = "B"
            else:
                overall_grade = "C"
            
            # Build summary
            summary_parts = []
            summary_parts.append(f"**ðŸ¤– AI ê¸°ìˆ **: {ai_tech.get('category', 'Unknown')}")
            summary_parts.append(f"\n\n**ðŸ’¡ í•œì¤„ ìš”ì•½**: {one_liner.get('summary', 'N/A')}")
            summary_parts.append(f"\n\n**âœ… ì¶”ì²œë„**: {recommendation}")
            if one_liner.get('recommendation_reason'):
                summary_parts.append(f"\n{one_liner.get('recommendation_reason')}")
            
            summary = "".join(summary_parts)
            
            # Update application
            application.ai_categories = ai_categories
            application.ai_category_primary = ai_tech.get("category", "Unknown")
            application.ai_evaluation_detail = evaluation_detail
            application.ai_grade = overall_grade
            application.ai_summary = summary
            application.ai_evaluated_at = datetime.utcnow()
            application.status = "ai_evaluated"
            
            # Save evaluation history
            history = EvaluationHistory(
                application_id=application.id,
                evaluator_id=None,
                evaluator_type="AI",
                grade=overall_grade,
                summary=summary,
                evaluation_detail=evaluation_detail,
                ai_categories=ai_categories
            )
            db.add(history)
            
            db.commit()
            print(f"âœ… Application {application.id} evaluated: {overall_grade} ({ai_tech.get('category', 'Unknown')})")
            return True
            
        except Exception as e:
            print(f"âŒ Error evaluating application {application.id}: {e}")
            import traceback
            traceback.print_exc()
            db.rollback()
            return False
    
    def _score_to_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 4.5:
            return "S"
        elif score >= 3.5:
            return "A"
        elif score >= 2.5:
            return "B"
        elif score >= 1.5:
            return "C"
        else:
            return "D"


# Singleton instance
llm_evaluator = LLMEvaluator()

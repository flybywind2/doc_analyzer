"""
LLM Evaluator Service
"""
import uuid
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from app.config import settings
from app.models.application import Application
from app.models.evaluation import EvaluationCriteria, EvaluationHistory
from app.services.rate_limiter import RateLimiter


class LLMEvaluator:
    """LLM-based application evaluator"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            base_url=settings.llm_api_base_url,
            api_key=settings.llm_api_key,
            model=settings.llm_model_name,
            temperature=0.1,  # ì¼ê´€ì„± í™•ë³´
            default_headers={
                "x-dep-ticket": settings.llm_credential_key,
                "Send-System-Name": settings.llm_system_name,
                "User-ID": settings.llm_user_id,
                "User-Type": "AD",
                "Prompt-Msg-Id": str(uuid.uuid4()),
                "Completion-Msg-Id": str(uuid.uuid4()),
            },
        )
        # Rate limiter: 20 calls per minute
        self.rate_limiter = RateLimiter(max_calls=20, time_window=60)
    
    def build_evaluation_prompt(
        self, 
        application: Application, 
        criteria_list: List[EvaluationCriteria]
    ) -> str:
        """
        Build evaluation prompt for LLM
        
        Args:
            application: Application to evaluate
            criteria_list: List of evaluation criteria
            
        Returns:
            Formatted prompt string
        """
        # ê³¼ì œ ì •ë³´ êµ¬ì„±
        department_info = f"{application.division or 'N/A'} > {application.department.name if application.department else 'N/A'}"
        
        system_prompt = f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ëŒ€ê¸°ì—…ì˜ AI ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì¡°ì§: {department_info}

ì—­í• : ì§€ì›ì„œ ë‚´ìš©ì„ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.

ì¤‘ìš” ì›ì¹™:
1. ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
2. {department_info} ì¡°ì§ì˜ ì—…ë¬´ íŠ¹ì„±ì„ ê³ ë ¤í•œ í•´ì„
3. ì‚¬ì‹¤ ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
4. ê³¼ìž¥í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ
"""

        app_info = f"""
# AI ê³¼ì œ ì§€ì›ì„œ í‰ê°€

## ê³¼ì œ ê¸°ë³¸ ì •ë³´
- ê³¼ì œëª…: {application.subject or 'N/A'}
- ì¡°ì§: {department_info}
- ì°¸ì—¬ ì¸ì›: {application.participant_count or 'N/A'}ëª…
- ëŒ€í‘œìž: {application.representative_name or 'N/A'}

## ì‹ ì²­ ë‚´ìš©
### í˜„ìž¬ ì—…ë¬´
{application.current_work or 'N/A'}

### Pain Point (í•´ê²°í•˜ê³ ìž í•˜ëŠ” ë¬¸ì œ)
{application.pain_point or 'N/A'}

### ê°œì„  ì•„ì´ë””ì–´
{application.improvement_idea or 'N/A'}

### ê¸°ëŒ€ íš¨ê³¼
{application.expected_effect or 'N/A'}

### ë°”ë¼ëŠ” ì 
{application.hope or 'N/A'}

## ì‚¬ì „ ì„¤ë¬¸
{json.dumps(application.pre_survey, ensure_ascii=False, indent=2) if application.pre_survey else 'N/A'}

## ì°¸ì—¬ìž ê¸°ìˆ  ì—­ëŸ‰
{json.dumps(application.tech_capabilities, ensure_ascii=False, indent=2) if application.tech_capabilities else 'N/A'}

---

## ìš”ì•½ ìš”ì²­ì‚¬í•­

ì§€ì›ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ 4ê°€ì§€ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”:

### 1. AI ê¸°ìˆ  ë¶„ë¥˜
ì§€ì›ì„œì—ì„œ ì–¸ê¸‰ëœ AI ê¸°ìˆ ì„ ë‹¤ìŒ ì¤‘ **í•˜ë‚˜ë§Œ** ì„ íƒí•˜ì„¸ìš”:
- **ì˜ˆì¸¡**: ë¯¸ëž˜ ê°’ ì˜ˆì¸¡, ìˆ˜ìš” ì˜ˆì¸¡, íŠ¸ë Œë“œ ë¶„ì„
- **ë¶„ë¥˜**: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ë¥˜, ë¶ˆëŸ‰ ê²€ì¶œ, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
- **ì±—ë´‡**: ëŒ€í™”í˜• ì¸í„°íŽ˜ì´ìŠ¤, ìžë™ ì‘ë‹µ, Q&A
- **ì—ì´ì „íŠ¸**: ìžìœ¨ ì˜ì‚¬ê²°ì •, ë³µìž¡í•œ ìž‘ì—… ìžë™í™”, ì›Œí¬í”Œë¡œìš° ìžë™í™”
- **ìµœì í™”**: ìžì› ìµœì í™”, ìŠ¤ì¼€ì¤„ë§, ê²½ë¡œ ìµœì í™”
- **ê°•í™”í•™ìŠµ**: í•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ê²°ì •, ì‹œë®¬ë ˆì´ì…˜ ìµœì í™”

### 2. ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼
{department_info} ì¡°ì§ ê´€ì ì—ì„œ ì´ ê³¼ì œì˜ ê²½ì˜íš¨ê³¼ë¥¼ ìš”ì•½í•˜ì„¸ìš” (2-3ë¬¸ìž¥):
- ì§€ì›ì„œì— ìž‘ì„±ëœ ê¸°ëŒ€íš¨ê³¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ìž‘ì„±
- ì¶”ì¸¡ì´ë‚˜ ê³¼ìž¥ ê¸ˆì§€

### 3. AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±
ì§€ì›ì„œ ë‚´ìš©(ì°¸ì—¬ì¸ì›, ê¸°ìˆ ì—­ëŸ‰, ë°ì´í„° ë“±)ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥ì„± í‰ê°€ (2-3ë¬¸ìž¥):
- ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œ ì°¸ê³ 
- ê¸°ìˆ ì  ë‚œì´ë„, ë°ì´í„° í™•ë³´, íŒ€ ì—­ëŸ‰ ë“±ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€

### 4. ì „ì²´ ì§€ì›ì„œ 5ì¤„ ìš”ì•½
ì´ ì§€ì›ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 5ì¤„ë¡œ ìš”ì•½:
1. ê³¼ì œ ëª©ì  (1ì¤„)
2. í˜„ìž¬ ë¬¸ì œ (1ì¤„)
3. í•´ê²° ë°©ì•ˆ (1ì¤„)
4. ê¸°ëŒ€ íš¨ê³¼ (1ì¤„)
5. êµ¬í˜„ ê³„íš (1ì¤„)
"""
        
        prompt = f"""{system_prompt}

{app_info}

---

## ì‘ë‹µ í˜•ì‹ (JSON)
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•ížˆ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "ai_category": "ì˜ˆì¸¡" ë˜ëŠ” "ë¶„ë¥˜" ë˜ëŠ” "ì±—ë´‡" ë˜ëŠ” "ì—ì´ì „íŠ¸" ë˜ëŠ” "ìµœì í™”" ë˜ëŠ” "ê°•í™”í•™ìŠµ",
  "business_impact": "ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼ë¥¼ 2-3ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½ (ì§€ì›ì„œ ë‚´ìš© ê¸°ë°˜)",
  "technical_feasibility": "AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±ì„ 2-3ë¬¸ìž¥ìœ¼ë¡œ í‰ê°€ (ì§€ì›ì„œ ë‚´ìš© ê¸°ë°˜)",
  "five_line_summary": [
    "1. ê³¼ì œ ëª©ì ",
    "2. í˜„ìž¬ ë¬¸ì œ",
    "3. í•´ê²° ë°©ì•ˆ",
    "4. ê¸°ëŒ€ íš¨ê³¼",
    "5. êµ¬í˜„ ê³„íš"
  ]
}}

**ì¤‘ìš” ê·œì¹™:**
1. ìœ íš¨í•œ JSON í˜•ì‹ í•„ìˆ˜
2. ai_categoryëŠ” 6ê°œ ì„ íƒì§€ ì¤‘ í•˜ë‚˜ë§Œ (ì˜ˆì¸¡/ë¶„ë¥˜/ì±—ë´‡/ì—ì´ì „íŠ¸/ìµœì í™”/ê°•í™”í•™ìŠµ)
3. ì§€ì›ì„œì— ìž‘ì„±ëœ ë‚´ìš©ë§Œ ì‚¬ìš© (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
4. ì¶”ì¸¡ì´ë‚˜ ê³¼ìž¥ ê¸ˆì§€ - ì‚¬ì‹¤ë§Œ ê¸°ë°˜
5. {department_info} ì¡°ì§ íŠ¹ì„± ë°˜ì˜
6. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ (ìš”ì•½ì˜ ëª©ì )
"""
        return prompt
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type(Exception)
    )
    def evaluate_with_llm(self, prompt: str) -> Dict[str, Any]:
        """
        Evaluate application using LLM with retry logic
        
        Args:
            prompt: Evaluation prompt
            
        Returns:
            Evaluation result dictionary
            
        Raises:
            Exception: If evaluation fails after retries
        """
        # Apply rate limiting before LLM call
        self.rate_limiter.wait_if_needed()
        
        response = self.llm.invoke(prompt)
        content = response.content
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            # Markdown code block ì œê±°
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            result = json.loads(content.strip())
            return result
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parsing error: {e}")
            print(f"Response content: {content}")
            raise
    
    def calculate_overall_grade(self, evaluation_detail: Dict[str, Any]) -> str:
        """
        Calculate overall grade from evaluation details
        
        Args:
            evaluation_detail: Dictionary of evaluation scores
            
        Returns:
            Overall grade (S/A/B/C/D)
        """
        total_score = 0
        count = 0
        
        for item in evaluation_detail.values():
            if isinstance(item, dict) and "score" in item:
                total_score += item["score"]
                count += 1
        
        if count == 0:
            return "C"
        
        avg_score = total_score / count
        
        if avg_score >= 4.5:
            return "S"
        elif avg_score >= 3.5:
            return "A"
        elif avg_score >= 2.5:
            return "B"
        elif avg_score >= 1.5:
            return "C"
        else:
            return "D"
    
    def evaluate_application(
        self, 
        db: Session, 
        application: Application,
        criteria_list: Optional[List[EvaluationCriteria]] = None
    ) -> bool:
        """
        Evaluate single application
        
        Args:
            db: Database session
            application: Application to evaluate
            criteria_list: Evaluation criteria (optional, will fetch if None)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Get evaluation criteria if not provided (backward compatibility)
            if criteria_list is None:
                criteria_list = db.query(EvaluationCriteria).filter(
                    EvaluationCriteria.is_active == True
                ).order_by(EvaluationCriteria.display_order).all()
            
            # Build prompt
            prompt = self.build_evaluation_prompt(application, criteria_list or [])
            
            # Evaluate with LLM
            print(f"ðŸ¤– Evaluating application {application.id} ({application.subject})...")
            result = self.evaluate_with_llm(prompt)
            
            # Extract simplified format results
            ai_category = result.get("ai_category", "ë¶„ë¥˜")
            business_impact = result.get("business_impact", "")
            technical_feasibility = result.get("technical_feasibility", "")
            five_line_summary = result.get("five_line_summary", [])
            
            # Build AI categories for compatibility
            ai_categories = [{
                "category": ai_category,
                "description": "ì§€ì›ì„œ ê¸°ë°˜ AI ìš”ì•½"
            }]
            
            # Build evaluation detail - simplified 4-item format
            evaluation_detail = {
                "ai_category": ai_category,
                "business_impact": business_impact,
                "technical_feasibility": technical_feasibility,
                "five_line_summary": five_line_summary
            }
            
            # Simple grade based on feasibility tone
            if "ì–´ë µ" in technical_feasibility or "ë¶ˆê°€ëŠ¥" in technical_feasibility:
                overall_grade = "C"
            elif "ê°€ëŠ¥" in technical_feasibility and "ì¶©ë¶„" in technical_feasibility:
                overall_grade = "A"
            else:
                overall_grade = "B"
            
            # Build summary
            summary_parts = []
            summary_parts.append(f"**AI ê¸°ìˆ  ë¶„ë¥˜**: {ai_category}\n\n")
            summary_parts.append(f"**ì¡°ì§ ê´€ì ì˜ ê²½ì˜íš¨ê³¼**\n{business_impact}\n\n")
            summary_parts.append(f"**AI ê´€ì ì˜ êµ¬í˜„ ê°€ëŠ¥ì„±**\n{technical_feasibility}\n\n")
            summary_parts.append(f"**ì „ì²´ ì§€ì›ì„œ 5ì¤„ ìš”ì•½**\n" + "\n".join(five_line_summary))
            
            summary = "".join(summary_parts)
            
            # Update application
            application.ai_categories = ai_categories
            application.ai_category_primary = ai_category
            application.ai_evaluation_detail = evaluation_detail
            application.ai_grade = overall_grade
            application.ai_summary = summary
            application.ai_evaluated_at = datetime.utcnow()
            application.status = "ai_evaluated"
            
            # Save evaluation history
            history = EvaluationHistory(
                application_id=application.id,
                evaluator_id=None,
                evaluator_type="AI",
                grade=overall_grade,
                summary=summary,
                evaluation_detail=evaluation_detail,
                ai_categories=ai_categories
            )
            db.add(history)
            
            db.commit()
            print(f"âœ… Application {application.id} evaluated: {overall_grade} ({ai_category})")
            return True
            
        except Exception as e:
            print(f"âŒ Error evaluating application {application.id}: {e}")
            import traceback
            traceback.print_exc()
            db.rollback()
            return False
    
    def _score_to_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 4.5:
            return "S"
        elif score >= 3.5:
            return "A"
        elif score >= 2.5:
            return "B"
        elif score >= 1.5:
            return "C"
        else:
            return "D"


# Singleton instance
llm_evaluator = LLMEvaluator()
